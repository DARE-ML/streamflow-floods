{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import folium\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from numpy import array\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "from keras.layers import TimeDistributed, RepeatVector\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from textwrap import wrap\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9189aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96023afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs = []\n",
    "summary_dfs = []\n",
    "\n",
    "# Read all csv files from directory\n",
    "# Sort files into timeseries and summary data\n",
    "for file_path in glob.glob('D:/Flood Deep Learning/**/*.csv', recursive=True):\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path, low_memory=False) \n",
    "\n",
    "    #skip these files\n",
    "    if file_name in ['streamflow_QualityCodes']:\n",
    "        continue\n",
    "\n",
    "    if 'year' in df.columns:    \n",
    "        df['source'] = file_name\n",
    "        df= df[df['year'] > 1990]\n",
    "        df= df.drop_duplicates(['year','month','day'])\n",
    "        timeseries_dfs.append(df)\n",
    "    else:\n",
    "        df = df.rename({'ID':'station_id'}, axis=1)\n",
    "        df = df.set_index('station_id')\n",
    "        summary_dfs.append(df)\n",
    "\n",
    "timeseries_data = pd.concat(timeseries_dfs, axis=0, ignore_index=True)\n",
    "timeseries_data['date'] = pd.to_datetime(timeseries_data[['year', 'month', 'day']])\n",
    "timeseries_data = timeseries_data.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "summary_data = pd.concat(summary_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c59bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self, timeseries_data=timeseries_data, summary_data=summary_data):\n",
    "        ### Data Cleaning\n",
    "        self.timeseries_data = timeseries_data.replace(-99.99,np.NaN)\n",
    "        \n",
    "        ### Feature Engineering\n",
    "        # get precipitation deficit\n",
    "        actualTransEvap_data = self.timeseries_data[self.timeseries_data['source'] == 'et_morton_actual_SILO'].drop(['source'], axis=1)\n",
    "        precipitation_data = self.timeseries_data[self.timeseries_data['source'] == 'precipitation_AWAP'].drop(['source'], axis=1)\n",
    "         \n",
    "        actualTransEvap_data = actualTransEvap_data[actualTransEvap_data['date'].isin(precipitation_data['date'])].reset_index(drop=True)\n",
    "        precipitation_data = precipitation_data[precipitation_data['date'].isin(actualTransEvap_data['date'])].reset_index(drop=True)\n",
    "        \n",
    "        self.precipitation_deficit = precipitation_data.drop(['date'], axis=1).subtract(actualTransEvap_data.drop(['date'], axis=1))\n",
    "        self.precipitation_deficit['source'] = 'precipitation_deficit'\n",
    "        self.precipitation_deficit['date'] = precipitation_data['date']\n",
    "        \n",
    "        # get flood probabilities\n",
    "        self.streamflow_data = self.timeseries_data[timeseries_data['source'] == 'streamflow_MLd_inclInfilled'].drop(['source'], axis=1)\n",
    "        self.streamflow_data = self.streamflow_data.set_index('date')\n",
    "        \n",
    "        self.flood_probabilities = self.streamflow_data.apply(self.flood_extent, axis=0)\n",
    "        self.flood_probabilities['source'] = 'flood_probabilities'\n",
    "        self.flood_probabilities['date'] = self.streamflow_data.index\n",
    "        \n",
    "        self.flood_indicator = self.flood_probabilities.applymap(lambda x: int(x <0.05) if pd.isnull(x) == False and isinstance(x, float) else x)\n",
    "        self.flood_indicator['source'] = 'flood_indicator'\n",
    "        self.flood_indicator['date'] = self.flood_probabilities['date']        \n",
    "        \n",
    "        # turn date into sin and cos function \n",
    "        date_min = np.min(self.flood_probabilities['date'])\n",
    "        year_seconds = 365.2425*24*60*60\n",
    "        year_sin = self.flood_probabilities['date'].apply(lambda x: np.sin((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        year_cos = self.flood_probabilities['date'].apply(lambda x: np.cos((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        all_stations = list(self.flood_probabilities.drop(columns=['source', 'date'], axis=1).columns) \n",
    "        \n",
    "        df_sin = []     \n",
    "        for value in year_sin:\n",
    "            df_sin.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_sin = pd.DataFrame(df_sin)\n",
    "        df_sin['source'] = 'year_sin'\n",
    "        df_sin['date'] = self.flood_probabilities['date']\n",
    " \n",
    "        df_cos = []\n",
    "        for value in year_cos:\n",
    "            df_cos.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_cos = pd.DataFrame(df_cos)\n",
    "        df_cos['source'] = 'year_cos'\n",
    "        df_cos['date'] = self.flood_probabilities['date']\n",
    "            \n",
    "        ### Return\n",
    "        self.timeseries_data = pd.concat([self.timeseries_data, self.precipitation_deficit, self.flood_probabilities, df_sin, df_cos, self.flood_indicator], axis=0).reset_index(drop=True)\n",
    "        self.summary_data = summary_data\n",
    "        \n",
    "    def get_timeseries_data(self, source, stations):      \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        return self.data_filtered\n",
    "        \n",
    "        \n",
    "    def get_data(self, source, stations):\n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]\n",
    "     \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(timeseries_source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                self.data_filtered[station, variable] = value\n",
    "        \n",
    "        return self.data_filtered.sort_index(axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_train_val_test(self, source, stations, \n",
    "                           scaled=True, target=['streamflow_MLd_inclInfilled'],\n",
    "                           start=None, end=None,\n",
    "                           discard=0.05, train=0.6, test=0.4):\n",
    "        assert 0<=discard<=1\n",
    "        assert (train + test) == 1\n",
    "     \n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]        \n",
    "        \n",
    "        all_data = self.get_timeseries_data(timeseries_source, stations).loc[start:end]\n",
    "        n_rows_all = len(all_data)\n",
    "        \n",
    "        all_data_discarded = all_data.iloc[int(n_rows_all*discard):]\n",
    "        n_rows_discarded = len(all_data_discarded)\n",
    "        \n",
    "        train_df = all_data_discarded[:int(n_rows_discarded*train)]\n",
    "        test_df = all_data_discarded[-int(n_rows_discarded*(test)):]\n",
    "        \n",
    "        if scaled == True:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_df)\n",
    "            \n",
    "            scaler_test = MinMaxScaler()\n",
    "            scaler_test.fit(test_df)\n",
    "            \n",
    "            train_df = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "            test_df = pd.DataFrame(scaler_test.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "            \n",
    "     \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                \n",
    "                train_df[station, variable] = value                \n",
    "                test_df[station, variable] = value \n",
    "                                  \n",
    "        return train_df.sort_index(axis=1), test_df.sort_index(axis=1) \n",
    "    \n",
    "    def flood_extent(self, streamflow_ts):\n",
    "        station_name = streamflow_ts.name\n",
    "\n",
    "        flow_data = pd.DataFrame(streamflow_ts)  \n",
    "        na_values = flow_data[flow_data[station_name].isna()][station_name]\n",
    "\n",
    "        flow_data = flow_data.dropna().sort_values(by=station_name, ascending=False).reset_index()\n",
    "        flow_data['probability'] = (flow_data.index + 1)/(1+len(flow_data)) \n",
    "        flow_data = flow_data.sort_values(by='date').drop(['date', station_name], axis=1)['probability']\n",
    "        flow_data = pd.concat([na_values, flow_data]).reset_index(drop=True) \n",
    "        flow_data.name = station_name  \n",
    "\n",
    "        return flow_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89db481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camels_data = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, test_df, \n",
    "                 station, filtered=None, label_columns=None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df[station]\n",
    "        self.test_df = test_df[station]\n",
    "        self.station = station\n",
    "        self.filtered = filtered\n",
    "        \n",
    "        # validation\n",
    "        if self.filtered == 'upper_soil_filter':\n",
    "            assert('upper_soil_indicator' in list(self.train_df.columns))\n",
    "        \n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(self.train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, filtered=None, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length=self.total_window_size,\n",
    "                sequence_stride=1,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        \n",
    "        if filtered == 'upper_soil_filter':\n",
    "            indicator_index = list(self.train_df.columns).index('upper_soil_indicator')\n",
    "            ds = ds.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, indicator_index]) > 0).batch(32)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df, filtered=self.filtered)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df, shuffle=False)\n",
    "           \n",
    "    @property\n",
    "    def test_windows(self):\n",
    "        total_size = self.test_array.shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    @property\n",
    "    def test_array(self):\n",
    "        return array(self.test_df[self.label_columns])\n",
    "       \n",
    "    def test_indicator(self, filtered):\n",
    "        if filtered == 'upper_soil_filter':\n",
    "            return array(self.test_df['upper_soil_indicator'])       \n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "            \n",
    "        return result\n",
    "       \n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self.train_df.shape[1]\n",
    "    \n",
    "    def test_example(self, index):\n",
    "        return np.array(self.test_df.iloc[index]).reshape(1, 1, self.num_features)\n",
    "\n",
    "    def plot(self, model=None, plot_col='streamflow_MLd_inclInfilled', max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                              marker='X', edgecolors='k', label='Predictions',\n",
    "                              c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ed6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df,test_df, stations, \n",
    "                 statics='separate', filtered=-1000, label_columns=None):\n",
    "     \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df       \n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations) \n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.filtered = filtered\n",
    "           \n",
    "        self.trains = []\n",
    "        self.tests = []        \n",
    "\n",
    "        for i, s in enumerate(stations):\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "\n",
    "            padding = np.zeros((input_width, self.total_stations), dtype=np.float32)\n",
    "            padding[:, i] = 1\n",
    "\n",
    "            self.trains.append(window.train.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            self.tests.append(window.test.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.trains)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE).filter(lambda x, y: tf.math.reduce_max(y) > self.filtered)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.tests)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])\n",
    "    \n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb25c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNumpyWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 timeseries_source, summary_source, summary_data,\n",
    "                 stations, label_columns=None):\n",
    "        \n",
    "        train_df, test_df = camels_data.get_train_val_test(source=timeseries_source,\n",
    "                                                                   stations=stations)      \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations)\n",
    "        \n",
    "        self.num_timeseries_features = len(timeseries_source)\n",
    "        self.num_static_features = len(summary_source)     \n",
    "        self.timeseries_source = timeseries_source\n",
    "        self.summary_source = summary_source      \n",
    "    \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.train_timeseries = []\n",
    "        self.test_timeseries = []  \n",
    "        \n",
    "        self.train_static = []\n",
    "        self.test_static = [] \n",
    "        \n",
    "        self.train_y = []\n",
    "        self.test_y = []\n",
    "        \n",
    "        for i, s in enumerate(stations):\n",
    "            # process timeseries\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "            \n",
    "            x_train, y_train = self.mapdata_tonumpy(window.train)\n",
    "            x_test, y_test = self.mapdata_tonumpy(window.test)\n",
    "            \n",
    "            self.train_timeseries.extend(x_train) \n",
    "            self.test_timeseries.extend(x_test) \n",
    "            \n",
    "            self.train_y.extend(y_train) \n",
    "            self.test_y.extend(y_test)      \n",
    "            \n",
    "            # process static\n",
    "            static = summary_data[summary_source].loc[s].to_numpy()         \n",
    "            padding = np.zeros((self.total_stations, ), dtype=np.float32)\n",
    "            padding[i] = 1\n",
    "            \n",
    "            static = np.concatenate([static, padding], axis=0)\n",
    "            \n",
    "            self.train_static.extend([static for _ in range(x_train.shape[0])])\n",
    "            self.test_static.extend([static for _ in range(x_test.shape[0])])\n",
    "            \n",
    "        self.train_timeseries = np.array(self.train_timeseries)\n",
    "        self.test_timeseries = np.array(self.test_timeseries) \n",
    "        \n",
    "        self.train_static = np.array(self.train_static)\n",
    "        self.test_static = np.array(self.test_static)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.train_static)\n",
    "        \n",
    "        self.train_static = scaler.transform(self.train_static)\n",
    "        self.test_static = scaler.transform(self.test_static)\n",
    "                \n",
    "        \n",
    "        #self.train_static = np.ones((self.train_static.shape[0], self.train_static.shape[1]))\n",
    "        \n",
    "        self.train_y = np.array(self.train_y) \n",
    "        self.train_y = np.swapaxes(self.train_y, 1, 2)\n",
    "\n",
    "        \n",
    "        self.test_y = np.array(self.test_y)\n",
    "        self.test_y = np.swapaxes(self.test_y, 1, 2)        \n",
    "\n",
    "    def mapdata_tonumpy(self, map_ds):\n",
    "        map_ds = map_ds.unbatch()\n",
    "\n",
    "        x_array = []\n",
    "        y_array = []\n",
    "\n",
    "        for ts in map_ds:\n",
    "            x = ts[0]\n",
    "            y= ts[1]\n",
    "\n",
    "            x_array.append(x.numpy())\n",
    "            y_array.append(y.numpy())\n",
    "\n",
    "        return np.array(x_array), np.array(y_array) \n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.train_timeseries, self.train_static, self.train_y\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):      \n",
    "        return self.test_timeseries, self.test_static, self.test_y\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40570b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss():  \n",
    "    def qloss_95(y_true, y_pred, q=0.95):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "        \n",
    "    def qloss_90(y_true, y_pred, q=0.9):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_70(y_true, y_pred, q=0.7):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_50(y_true, y_pred, q=0.5):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "class Model():\n",
    "    MAX_EPOCHS = 150\n",
    "    \n",
    "    def __init__(self, window):\n",
    "        # Store the raw data.\n",
    "        self.window = window\n",
    "        \n",
    "        self.train_df = self.window.train_df\n",
    "        self.test_df = self.window.test_df\n",
    "             \n",
    "    def compile_and_fit(self, model, window, loss_func, patience=10):\n",
    "\n",
    "        model.compile(loss=loss_func,\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = model.fit(window.train, epochs=self.MAX_EPOCHS,\n",
    "                            verbose=0)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def num_flood_events(self, cut=1):\n",
    "        actuals = np.squeeze(self.window.test_windows, axis=2) \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "        locs = np.unique(np.where(actuals<cut_percentile)[0])\n",
    "\n",
    "        events = np.split(locs, np.cumsum( np.where(locs[1:] - locs[:-1] > 1) )+1)\n",
    "\n",
    "        return len(events)\n",
    "    \n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['model_name'] = self.model_name\n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "             \n",
    "        if station == None:\n",
    "            summary_dict['station'] = self.window.station\n",
    "            summary_dict['inputs'] = str(list(self.train_df.columns))\n",
    "            summary_dict['NSE'] = self.get_NSE()\n",
    "        else:\n",
    "            summary_dict['station'] = station\n",
    "            example_station = self.train_df.columns.get_level_values(0)[0]\n",
    "            summary_dict['inputs'] = str(list(self.train_df[example_station].columns))\n",
    "            summary_dict['NSE'] = self.get_NSE(station)  \n",
    "                             \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "\n",
    "        summary_dict['f1_score_individual_1%'] = self.binary_metrics(station=station, cut=1, metric='f1_score', evaluation='individual')        \n",
    "        summary_dict['f1_score_individual_2%'] = self.binary_metrics(station=station, cut=2, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_5%'] = self.binary_metrics(station=station, cut=5, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_10%'] = self.binary_metrics(station=station, cut=10, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_25%'] = self.binary_metrics(station=station, cut=25, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_50%'] = self.binary_metrics(station=station, cut=50, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_75%'] = self.binary_metrics(station=station, cut=75, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_all'] = self.binary_metrics(station=station, cut=100, metric='f1_score', evaluation='individual') \n",
    "          \n",
    "        return summary_dict\n",
    "            \n",
    "    def print_model_error(self, station=None, cut=0):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "            test_array = self.window.test_array(station)\n",
    "        else:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows  \n",
    "            test_array = self.window.test_array\n",
    "            \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.unique(np.where(actuals>cut_percentile)[0])\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        for window_pred, window_actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(test_array[loc:loc+self.window.input_width].flatten()))\n",
    "            print(\"Predicted: {}\".format(window_pred))\n",
    "            print(\"Actual: {}\".format(window_actual))\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "    def model_predictions_less_than_cut(self, cut=100):\n",
    "        \n",
    "        preds = self.predictions\n",
    "        actuals =self.window.test_windows\n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        num_predicted = (preds.flatten() < cut_percentile).sum()\n",
    "        num_actual = (actuals.flatten() < cut_percentile).sum()\n",
    "\n",
    "        return num_predicted, num_actual\n",
    "        \n",
    "    def average_model_error(self, station=None, cut=100):\n",
    "        if self.window.label_columns[0] == 'streamflow_MLd_inclInfilled':\n",
    "            cut = 100 - cut\n",
    "            \n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows         \n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.where(actuals>cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "\n",
    "        return avg_error\n",
    "    \n",
    "    def get_NSE(self, station=None, type='cast'):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows\n",
    "        \n",
    "        NSE = []\n",
    "\n",
    "        for i in range(self.window.label_width):\n",
    "            numer = np.sum(np.square(preds[:, i] - actuals[:, i]))\n",
    "            denom = np.sum(np.square(actuals[:, i] - np.mean(actuals[:, i])))\n",
    "        \n",
    "            NSE.append(1-(numer/denom))\n",
    "        \n",
    "        if type=='cast':\n",
    "            return np.mean(NSE)\n",
    "        else:\n",
    "            return NSE\n",
    "\n",
    "    def binary_metrics(self, cut, metric, evaluation='whole', station=None):\n",
    "        percentile_cut = self.window.station_percentile(station=station, cut=cut)\n",
    "        \n",
    "        if station==None:\n",
    "            preds_pre = self.predictions()\n",
    "            actuals_pre = self.window.test_windows\n",
    "        else:        \n",
    "            preds_pre = self.predictions(station)\n",
    "            actuals_pre = self.window.test_windows(station)\n",
    "            \n",
    "        if evaluation=='whole':  \n",
    "            preds = np.array([int(any(x > percentile_cut)) for x in preds_pre])\n",
    "            actuals = np.array([int(any(x > percentile_cut)) for x in actuals_pre])\n",
    "        else:\n",
    "            preds = np.array([int(x > percentile_cut) for x in preds_pre.flatten()])           \n",
    "            actuals = np.array([int(x > percentile_cut) for x in actuals_pre.flatten()])\n",
    "\n",
    "        if metric=='accuracy':\n",
    "            return accuracy_score(actuals, preds)\n",
    "        elif metric=='precision':\n",
    "            return precision_score(actuals, preds)\n",
    "        elif metric=='recall':\n",
    "            return recall_score(actuals, preds)\n",
    "        elif metric=='f1_score':\n",
    "            return f1_score(actuals, preds)\n",
    "     \n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "\n",
    "    def predictions(self, station=None):\n",
    "        tf_test = self.window.test\n",
    "\n",
    "        if station != None:\n",
    "            filter_index = self.window.stations.index(station)\n",
    "            num_inputs = len(self.window.train_df.columns.levels[1])\n",
    "            tf_test = tf_test.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, num_inputs + filter_index]) > 0).batch(32)\n",
    "\n",
    "        return np.squeeze(self.model.predict(tf_test), axis=2)\n",
    "        \n",
    "class Base_Model(Model):  \n",
    "    def __init__(self, model_name, window, CONV_WIDTH, output_activation='sigmoid', loss_func=tf.losses.MeanSquaredError()):\n",
    "        super().__init__(window)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.mix_type_name = None\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        if self.model_name == 'multi-linear':          \n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Take the last time step.\n",
    "                            # Shape [batch, time, features] => [batch, 1, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "                            # Shape => [batch, 1, dense_units]\n",
    "                            tf.keras.layers.Dense(20, activation='relu'),\n",
    "                            # Shape => [batch, out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-CNN':\n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "                            # Shape => [batch, 1, conv_units]\n",
    "                            tf.keras.layers.Conv1D(64, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "                            # Shape => [batch, 1,  out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=False),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "\n",
    "            \n",
    "        elif self.model_name == 'multi-ED-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=True,),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dropout(0.2),\n",
    "                            Flatten(),\n",
    "                            RepeatVector(5),\n",
    "                            LSTM(20, return_sequences=False), \n",
    "                            \n",
    "                            Dropout(0.2),                \n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])    \n",
    "            \n",
    "        elif self.model_name == 'multi-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(20, return_sequences=False)),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])  \n",
    "            \n",
    "        elif self.model_name == 'multi-deep-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(64, return_sequences=True\n",
    "                                              )),\n",
    "                            Dropout(0.2),\n",
    "                            Bidirectional(LSTM(32, return_sequences=False)),\n",
    "                            Dropout(0.2),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ]) \n",
    "            \n",
    "        self.compile_and_fit(self.model, window, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82744dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixed_Model(Base_Model):\n",
    "    threshold = 0.2\n",
    "    \n",
    "    def __init__(self, model_name, mix_type_name, window, CONV_WIDTH):\n",
    "        super().__init__(model_name, window, CONV_WIDTH)\n",
    "        self.mix_type_name = mix_type_name\n",
    "               \n",
    "        if self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=1,\n",
    "                                             shift=1,\n",
    "                                             train_df=train_df.loc[:,train_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             test_df=test_df.loc[:,test_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=1)\n",
    "            \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=self.window.input_width,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                            filtered='upper_soil_filter',\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        if self.mix_type_name == 'simple':\n",
    "            preds = super().predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            new_pred=[]\n",
    "       \n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = np.full((self.window.label_width,), actual_before)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = []\n",
    "                                      \n",
    "                    input_value = np.array(actual_before).reshape(1,1,1)\n",
    "                    \n",
    "                    for j in range(self.window.label_width):\n",
    "                        pred_simple = self.model_simple.model.predict(input_value).item()\n",
    "                        pred.append(pred_simple)\n",
    "                        \n",
    "                        input_value = np.array(pred_simple).reshape(1,1,1)\n",
    "                                         \n",
    "                    pred = np.array(pred)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, actual_before) in enumerate(zip(preds, test_array)):\n",
    "                if actual_before < self.threshold:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # upper soil indicator 1 time unit before predictions\n",
    "            upper_soil_indicator = window.test_indicator(filtered='upper_soil_filter')\n",
    "                      \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, indicator) in enumerate(zip(preds, upper_soil_indicator)):\n",
    "                if indicator == 1:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)              \n",
    "            \n",
    "            return np.array(new_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5344ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without Changes\n",
    "\n",
    "class Ensemble_Static():\n",
    "    epochs = 100\n",
    "    patience = 5\n",
    "    def __init__(self, numpy_window, batch_size=32):\n",
    "        num_timesteps = numpy_window.input_width\n",
    "        num_timeseries_features = numpy_window.num_timeseries_features\n",
    "        num_static_features = numpy_window.num_static_features + numpy_window.total_stations\n",
    "          \n",
    "        num_predictions = numpy_window.label_width\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.stations = numpy_window.stations\n",
    "        self.n_stations = numpy_window.total_stations\n",
    "        self.numpy_window = numpy_window\n",
    "        # RNN + SLP Model\n",
    "        # Define input layer\n",
    "\n",
    "        recurrent_input = Input(shape=(num_timesteps, num_timeseries_features),name=\"TIMESERIES_INPUT\")\n",
    "        static_input = Input(shape=(num_static_features,),name=\"STATIC_INPUT\")\n",
    "\n",
    "        # RNN Layers\n",
    "        # layer - 1\n",
    "        rec_layer_one = LSTM(20, name =\"BIDIRECTIONAL_LAYER_1\", return_sequences=True)(recurrent_input)\n",
    "        rec_layer_one = Dropout(0.1,name =\"DROPOUT_LAYER_1\")(rec_layer_one)\n",
    "        \n",
    "        # layer - 2\n",
    "        rec_layer_two = LSTM(20, name =\"BIDIRECTIONAL_LAYER_2\", return_sequences=False)(rec_layer_one)\n",
    "        rec_layer_two = Dropout(0.1,name =\"DROPOUT_LAYER_2\")(rec_layer_two)      \n",
    "        \n",
    "\n",
    "        # SLP Layers\n",
    "        static_layer_one = Dense(20, activation='relu',name=\"DENSE_LAYER_1\")(static_input)\n",
    "        # Combine layers - RNN + SLP\n",
    "        combined = Concatenate(axis= 1,name = \"CONCATENATED_TIMESERIES_STATIC\")([rec_layer_two, static_layer_one])\n",
    "        combined_dense_two = Dense(20, activation='relu',name=\"DENSE_LAYER_2\")(combined)\n",
    "        output = Dense(num_predictions, name=\"OUTPUT_LAYER\", activation='sigmoid')(combined_dense_two)\n",
    "\n",
    "      \n",
    "        # Compile ModeL\n",
    "        self.model = keras.models.Model(inputs=[recurrent_input, static_input], outputs=[output])\n",
    "        # MSE\n",
    "        \n",
    "        #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self.train_timeseries_x, self.train_static_x, self.train_y = numpy_window.train     \n",
    "        self.test_timeseries_x, self.test_static_x, self.test_y = numpy_window.test \n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['MeanAbsoluteError'])\n",
    "        \n",
    "        \n",
    "        self.model.fit([self.train_timeseries_x, self.train_static_x], \n",
    "                       self.train_y, \n",
    "                       epochs=self.epochs, \n",
    "                       batch_size=self.batch_size, \n",
    "                       verbose=1)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "    \n",
    "    def predictions(self, station):  \n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        \n",
    "        print(\"Timeseries input shape:\", self.test_timeseries_x[start:end].shape)\n",
    "        print(\"Static input shape:\", self.test_static_x[start:end].shape)\n",
    "        \n",
    "        return self.model.predict([self.test_timeseries_x[start:end,:], self.test_static_x[start:end,:]])\n",
    "    \n",
    "    \n",
    "    def actuals(self, station):\n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        print('TSET Y SHAPE',self.test_y.shape)\n",
    "        \n",
    "\n",
    "        return self.test_y.reshape(self.test_y.shape[0], self.test_y.shape[2])[start:end, :]\n",
    "\n",
    "    \n",
    "    def average_model_error(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        \n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "        if avg_error==0:\n",
    "            return 0\n",
    "        print(avg_error)\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "        return avg_error \n",
    "    \n",
    "    def print_model_windows(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "        \n",
    "        for pred, actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(self.test_y[loc:loc+self.numpy_window.input_width+1].flatten()))\n",
    "            print(\"Predicted: {}\".format(pred))\n",
    "            print(\"Actual: {}\".format(actual))\n",
    "            print(\"-------------------------\")        \n",
    "\n",
    "    def summary(self, station):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "        summary_dict['input_width'] = self.numpy_window.input_width\n",
    "        summary_dict['label_width'] = self.numpy_window.label_width\n",
    "        summary_dict['num_timeseries_features'] = self.numpy_window.num_timeseries_features \n",
    "        summary_dict['num_static_features'] = self.numpy_window.num_static_features        \n",
    "        summary_dict['timeseries_inputs'] = self.numpy_window.timeseries_source\n",
    "        summary_dict['static_inputs'] = self.numpy_window.summary_source     \n",
    "\n",
    "   \n",
    "        summary_dict['SERA_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SERA_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SERA_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SERA_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SERA_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SERA_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SERA_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['SERA_all'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea442a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Switch_Model(Model):\n",
    "    threshold = 0.7\n",
    "    \n",
    "    def __init__(self, window_switch, window_regular, CONV_WIDTH):\n",
    "        self.window_switch = window_switch\n",
    "        self.window = window_regular\n",
    "        \n",
    "        assert(window_switch.input_width == self.window.input_width)\n",
    "        \n",
    "        self.switch = Ensemble_Static(window_switch)\n",
    "        \n",
    "        self.regular = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH)\n",
    "        self.q70 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_70)\n",
    "        self.q95 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_95)\n",
    "        \n",
    "    def predictions(self, station):\n",
    "        preds_switch = self.switch.predictions(station)     \n",
    "        \n",
    "        preds_regular = self.regular.predictions(station)\n",
    "        preds_q70 = self.q70.predictions(station)        \n",
    "        preds_q95 = self.q95.predictions(station)\n",
    "        \n",
    "        test_array = self.window.test_windows(station)   \n",
    "\n",
    "        new_pred=[]\n",
    "        \n",
    "        for pred_switch, pred_regular, pred_q70, pred_q95 in zip(preds_switch, preds_regular, preds_q70, preds_q95):\n",
    "\n",
    "                \n",
    "            switch_condition = pred_switch > 0.95\n",
    "            q95_condition = pred_switch > 0.7\n",
    "            q70_condition = pred_switch <= 0.7  # You might want to specify this condition differently\n",
    "\n",
    "            new_pred.append(np.where(switch_condition, pred_q95, np.where(q95_condition, pred_q70, pred_regular)))\n",
    "                \n",
    "        return np.array(new_pred)\n",
    "        \n",
    "\n",
    "    def test_MSE(self, station=None):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = self.window.test_array(station)[self.window.input_width:]\n",
    "\n",
    "        return mean_squared_error(test_array, preds)\n",
    "    \n",
    "    def test_ROCAUC(self, station, level=0.05):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = (self.window.test_array(station)[self.window.input_width:] < level).astype(int)\n",
    "        \n",
    "        return roc_auc_score(test_array, preds)\n",
    "\n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "\n",
    "        summary_dict['NSE'] = self.get_NSE(station)       \n",
    "                  \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        return summary_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e435a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_data.summary_data= camels_data.summary_data.T.drop_duplicates().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
