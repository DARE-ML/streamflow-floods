{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9dace5",
   "metadata": {},
   "source": [
    "### Defining Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40570b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss():  \n",
    "    def qloss_95(y_true, y_pred, q=0.95):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "        \n",
    "    def qloss_90(y_true, y_pred, q=0.9):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_70(y_true, y_pred, q=0.7):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_50(y_true, y_pred, q=0.5):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "class Model():\n",
    "    MAX_EPOCHS = 150\n",
    "    \n",
    "    def __init__(self, window):\n",
    "        # Store the raw data.\n",
    "        self.window = window\n",
    "        \n",
    "        self.train_df = self.window.train_df\n",
    "        self.test_df = self.window.test_df\n",
    "             \n",
    "    def compile_and_fit(self, model, window, loss_func, patience=10):\n",
    "\n",
    "        model.compile(loss=loss_func,\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = model.fit(window.train, epochs=self.MAX_EPOCHS,\n",
    "                            verbose=0)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def num_flood_events(self, cut=1):\n",
    "        actuals = np.squeeze(self.window.test_windows, axis=2) \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "        locs = np.unique(np.where(actuals<cut_percentile)[0])\n",
    "\n",
    "        events = np.split(locs, np.cumsum( np.where(locs[1:] - locs[:-1] > 1) )+1)\n",
    "\n",
    "        return len(events)\n",
    "    \n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['model_name'] = self.model_name\n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "             \n",
    "        if station == None:\n",
    "            summary_dict['station'] = self.window.station\n",
    "            summary_dict['inputs'] = str(list(self.train_df.columns))\n",
    "            summary_dict['NSE'] = self.get_NSE()\n",
    "        else:\n",
    "            summary_dict['station'] = station\n",
    "            example_station = self.train_df.columns.get_level_values(0)[0]\n",
    "            summary_dict['inputs'] = str(list(self.train_df[example_station].columns))\n",
    "            summary_dict['NSE'] = self.get_NSE(station)  \n",
    "                             \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "\n",
    "        summary_dict['f1_score_individual_1%'] = self.binary_metrics(station=station, cut=1, metric='f1_score', evaluation='individual')        \n",
    "        summary_dict['f1_score_individual_2%'] = self.binary_metrics(station=station, cut=2, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_5%'] = self.binary_metrics(station=station, cut=5, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_10%'] = self.binary_metrics(station=station, cut=10, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_25%'] = self.binary_metrics(station=station, cut=25, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_50%'] = self.binary_metrics(station=station, cut=50, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_75%'] = self.binary_metrics(station=station, cut=75, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_all'] = self.binary_metrics(station=station, cut=100, metric='f1_score', evaluation='individual') \n",
    "          \n",
    "        return summary_dict\n",
    "            \n",
    "    def print_model_error(self, station=None, cut=0):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "            test_array = self.window.test_array(station)\n",
    "        else:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows  \n",
    "            test_array = self.window.test_array\n",
    "            \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.unique(np.where(actuals>cut_percentile)[0])\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        for window_pred, window_actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(test_array[loc:loc+self.window.input_width].flatten()))\n",
    "            print(\"Predicted: {}\".format(window_pred))\n",
    "            print(\"Actual: {}\".format(window_actual))\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "    def model_predictions_less_than_cut(self, cut=100):\n",
    "        \n",
    "        preds = self.predictions\n",
    "        actuals =self.window.test_windows\n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        num_predicted = (preds.flatten() < cut_percentile).sum()\n",
    "        num_actual = (actuals.flatten() < cut_percentile).sum()\n",
    "\n",
    "        return num_predicted, num_actual\n",
    "        \n",
    "    def average_model_error(self, station=None, cut=100):\n",
    "        if self.window.label_columns[0] == 'streamflow_MLd_inclInfilled':\n",
    "            cut = 100 - cut\n",
    "            \n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows         \n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.where(actuals>cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "\n",
    "        return avg_error\n",
    "    \n",
    "    def get_NSE(self, station=None, type='cast'):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows\n",
    "        \n",
    "        NSE = []\n",
    "\n",
    "        for i in range(self.window.label_width):\n",
    "            numer = np.sum(np.square(preds[:, i] - actuals[:, i]))\n",
    "            denom = np.sum(np.square(actuals[:, i] - np.mean(actuals[:, i])))\n",
    "        \n",
    "            NSE.append(1-(numer/denom))\n",
    "        \n",
    "        if type=='cast':\n",
    "            return np.mean(NSE)\n",
    "        else:\n",
    "            return NSE\n",
    "\n",
    "    def binary_metrics(self, cut, metric, evaluation='whole', station=None):\n",
    "        percentile_cut = self.window.station_percentile(station=station, cut=cut)\n",
    "        \n",
    "        if station==None:\n",
    "            preds_pre = self.predictions()\n",
    "            actuals_pre = self.window.test_windows\n",
    "        else:        \n",
    "            preds_pre = self.predictions(station)\n",
    "            actuals_pre = self.window.test_windows(station)\n",
    "            \n",
    "        if evaluation=='whole':  \n",
    "            preds = np.array([int(any(x > percentile_cut)) for x in preds_pre])\n",
    "            actuals = np.array([int(any(x > percentile_cut)) for x in actuals_pre])\n",
    "        else:\n",
    "            preds = np.array([int(x > percentile_cut) for x in preds_pre.flatten()])           \n",
    "            actuals = np.array([int(x > percentile_cut) for x in actuals_pre.flatten()])\n",
    "\n",
    "        if metric=='accuracy':\n",
    "            return accuracy_score(actuals, preds)\n",
    "        elif metric=='precision':\n",
    "            return precision_score(actuals, preds)\n",
    "        elif metric=='recall':\n",
    "            return recall_score(actuals, preds)\n",
    "        elif metric=='f1_score':\n",
    "            return f1_score(actuals, preds)\n",
    "     \n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "\n",
    "    def predictions(self, station=None):\n",
    "        tf_test = self.window.test\n",
    "\n",
    "        if station != None:\n",
    "            filter_index = self.window.stations.index(station)\n",
    "            num_inputs = len(self.window.train_df.columns.levels[1])\n",
    "            tf_test = tf_test.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, num_inputs + filter_index]) > 0).batch(32)\n",
    "\n",
    "        return np.squeeze(self.model.predict(tf_test), axis=2)\n",
    "        \n",
    "class Base_Model(Model):  \n",
    "    def __init__(self, model_name, window, CONV_WIDTH, output_activation='sigmoid', loss_func=tf.losses.MeanSquaredError()):\n",
    "        super().__init__(window)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.mix_type_name = None\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        if self.model_name == 'multi-linear':          \n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Take the last time step.\n",
    "                            # Shape [batch, time, features] => [batch, 1, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "                            # Shape => [batch, 1, dense_units]\n",
    "                            tf.keras.layers.Dense(20, activation='relu'),\n",
    "                            # Shape => [batch, out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-CNN':\n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "                            # Shape => [batch, 1, conv_units]\n",
    "                            tf.keras.layers.Conv1D(64, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "                            # Shape => [batch, 1,  out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=False),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "\n",
    "            \n",
    "        elif self.model_name == 'multi-ED-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=True,),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dropout(0.2),\n",
    "                            Flatten(),\n",
    "                            RepeatVector(5),\n",
    "                            LSTM(20, return_sequences=False), \n",
    "                            \n",
    "                            Dropout(0.2),                \n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])    \n",
    "            \n",
    "        elif self.model_name == 'multi-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(20, return_sequences=False)),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])  \n",
    "            \n",
    "        elif self.model_name == 'multi-deep-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(64, return_sequences=True\n",
    "                                              )),\n",
    "                            Dropout(0.2),\n",
    "                            Bidirectional(LSTM(32, return_sequences=False)),\n",
    "                            Dropout(0.2),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ]) \n",
    "            \n",
    "        self.compile_and_fit(self.model, window, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82744dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixed_Model(Base_Model):\n",
    "    threshold = 0.2\n",
    "    \n",
    "    def __init__(self, model_name, mix_type_name, window, CONV_WIDTH):\n",
    "        super().__init__(model_name, window, CONV_WIDTH)\n",
    "        self.mix_type_name = mix_type_name\n",
    "               \n",
    "        if self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=1,\n",
    "                                             shift=1,\n",
    "                                             train_df=train_df.loc[:,train_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             test_df=test_df.loc[:,test_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=1)\n",
    "            \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=self.window.input_width,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                            filtered='upper_soil_filter',\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        if self.mix_type_name == 'simple':\n",
    "            preds = super().predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            new_pred=[]\n",
    "       \n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = np.full((self.window.label_width,), actual_before)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = []\n",
    "                                      \n",
    "                    input_value = np.array(actual_before).reshape(1,1,1)\n",
    "                    \n",
    "                    for j in range(self.window.label_width):\n",
    "                        pred_simple = self.model_simple.model.predict(input_value).item()\n",
    "                        pred.append(pred_simple)\n",
    "                        \n",
    "                        input_value = np.array(pred_simple).reshape(1,1,1)\n",
    "                                         \n",
    "                    pred = np.array(pred)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, actual_before) in enumerate(zip(preds, test_array)):\n",
    "                if actual_before < self.threshold:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # upper soil indicator 1 time unit before predictions\n",
    "            upper_soil_indicator = window.test_indicator(filtered='upper_soil_filter')\n",
    "                      \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, indicator) in enumerate(zip(preds, upper_soil_indicator)):\n",
    "                if indicator == 1:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)              \n",
    "            \n",
    "            return np.array(new_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5344ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without Changes\n",
    "\n",
    "class Ensemble_Static():\n",
    "    epochs = 100\n",
    "    patience = 5\n",
    "    def __init__(self, numpy_window, batch_size=32):\n",
    "        num_timesteps = numpy_window.input_width\n",
    "        num_timeseries_features = numpy_window.num_timeseries_features\n",
    "        num_static_features = numpy_window.num_static_features + numpy_window.total_stations\n",
    "          \n",
    "        num_predictions = numpy_window.label_width\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.stations = numpy_window.stations\n",
    "        self.n_stations = numpy_window.total_stations\n",
    "        self.numpy_window = numpy_window\n",
    "        # RNN + SLP Model\n",
    "        # Define input layer\n",
    "\n",
    "        recurrent_input = Input(shape=(num_timesteps, num_timeseries_features),name=\"TIMESERIES_INPUT\")\n",
    "        static_input = Input(shape=(num_static_features,),name=\"STATIC_INPUT\")\n",
    "\n",
    "        # RNN Layers\n",
    "        # layer - 1\n",
    "        rec_layer_one = LSTM(20, name =\"BIDIRECTIONAL_LAYER_1\", return_sequences=True)(recurrent_input)\n",
    "        rec_layer_one = Dropout(0.1,name =\"DROPOUT_LAYER_1\")(rec_layer_one)\n",
    "        \n",
    "        # layer - 2\n",
    "        rec_layer_two = LSTM(20, name =\"BIDIRECTIONAL_LAYER_2\", return_sequences=False)(rec_layer_one)\n",
    "        rec_layer_two = Dropout(0.1,name =\"DROPOUT_LAYER_2\")(rec_layer_two)      \n",
    "        \n",
    "\n",
    "        # SLP Layers\n",
    "        static_layer_one = Dense(20, activation='relu',name=\"DENSE_LAYER_1\")(static_input)\n",
    "        # Combine layers - RNN + SLP\n",
    "        combined = Concatenate(axis= 1,name = \"CONCATENATED_TIMESERIES_STATIC\")([rec_layer_two, static_layer_one])\n",
    "        combined_dense_two = Dense(20, activation='relu',name=\"DENSE_LAYER_2\")(combined)\n",
    "        output = Dense(num_predictions, name=\"OUTPUT_LAYER\", activation='sigmoid')(combined_dense_two)\n",
    "\n",
    "      \n",
    "        # Compile ModeL\n",
    "        self.model = keras.models.Model(inputs=[recurrent_input, static_input], outputs=[output])\n",
    "        # MSE\n",
    "        \n",
    "        #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self.train_timeseries_x, self.train_static_x, self.train_y = numpy_window.train     \n",
    "        self.test_timeseries_x, self.test_static_x, self.test_y = numpy_window.test \n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['MeanAbsoluteError'])\n",
    "        \n",
    "        \n",
    "        self.model.fit([self.train_timeseries_x, self.train_static_x], \n",
    "                       self.train_y, \n",
    "                       epochs=self.epochs, \n",
    "                       batch_size=self.batch_size, \n",
    "                       verbose=1)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "    \n",
    "    def predictions(self, station):  \n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        \n",
    "        print(\"Timeseries input shape:\", self.test_timeseries_x[start:end].shape)\n",
    "        print(\"Static input shape:\", self.test_static_x[start:end].shape)\n",
    "        \n",
    "        return self.model.predict([self.test_timeseries_x[start:end,:], self.test_static_x[start:end,:]])\n",
    "    \n",
    "    \n",
    "    def actuals(self, station):\n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        print('TSET Y SHAPE',self.test_y.shape)\n",
    "        \n",
    "\n",
    "        return self.test_y.reshape(self.test_y.shape[0], self.test_y.shape[2])[start:end, :]\n",
    "\n",
    "    \n",
    "    def average_model_error(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        \n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "        if avg_error==0:\n",
    "            return 0\n",
    "        print(avg_error)\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "        return avg_error \n",
    "    \n",
    "    def print_model_windows(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "        \n",
    "        for pred, actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(self.test_y[loc:loc+self.numpy_window.input_width+1].flatten()))\n",
    "            print(\"Predicted: {}\".format(pred))\n",
    "            print(\"Actual: {}\".format(actual))\n",
    "            print(\"-------------------------\")        \n",
    "\n",
    "    def summary(self, station):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "        summary_dict['input_width'] = self.numpy_window.input_width\n",
    "        summary_dict['label_width'] = self.numpy_window.label_width\n",
    "        summary_dict['num_timeseries_features'] = self.numpy_window.num_timeseries_features \n",
    "        summary_dict['num_static_features'] = self.numpy_window.num_static_features        \n",
    "        summary_dict['timeseries_inputs'] = self.numpy_window.timeseries_source\n",
    "        summary_dict['static_inputs'] = self.numpy_window.summary_source     \n",
    "\n",
    "   \n",
    "        summary_dict['SERA_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SERA_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SERA_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SERA_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SERA_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SERA_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SERA_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['SERA_all'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Switch_Model(Model):\n",
    "    threshold = 0.7\n",
    "    \n",
    "    def __init__(self, window_switch, window_regular, CONV_WIDTH):\n",
    "        self.window_switch = window_switch\n",
    "        self.window = window_regular\n",
    "        \n",
    "        assert(window_switch.input_width == self.window.input_width)\n",
    "        \n",
    "        self.switch = Ensemble_Static(window_switch)\n",
    "        \n",
    "        self.regular = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH)\n",
    "        self.q70 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_70)\n",
    "        self.q95 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_95)\n",
    "        \n",
    "    def predictions(self, station):\n",
    "        preds_switch = self.switch.predictions(station)     \n",
    "        \n",
    "        preds_regular = self.regular.predictions(station)\n",
    "        preds_q70 = self.q70.predictions(station)        \n",
    "        preds_q95 = self.q95.predictions(station)\n",
    "        \n",
    "        test_array = self.window.test_windows(station)   \n",
    "\n",
    "        new_pred=[]\n",
    "        \n",
    "        for pred_switch, pred_regular, pred_q70, pred_q95 in zip(preds_switch, preds_regular, preds_q70, preds_q95):\n",
    "\n",
    "                \n",
    "            switch_condition = pred_switch > 0.95\n",
    "            q95_condition = pred_switch > 0.7\n",
    "            q70_condition = pred_switch <= 0.7  # You might want to specify this condition differently\n",
    "\n",
    "            new_pred.append(np.where(switch_condition, pred_q95, np.where(q95_condition, pred_q70, pred_regular)))\n",
    "                \n",
    "        return np.array(new_pred)\n",
    "        \n",
    "\n",
    "    def test_MSE(self, station=None):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = self.window.test_array(station)[self.window.input_width:]\n",
    "\n",
    "        return mean_squared_error(test_array, preds)\n",
    "    \n",
    "    def test_ROCAUC(self, station, level=0.05):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = (self.window.test_array(station)[self.window.input_width:] < level).astype(int)\n",
    "        \n",
    "        return roc_auc_score(test_array, preds)\n",
    "\n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "\n",
    "        summary_dict['NSE'] = self.get_NSE(station)       \n",
    "                  \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        return summary_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e435a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_data.summary_data= camels_data.summary_data.T.drop_duplicates().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
