{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7be2ddb",
   "metadata": {},
   "source": [
    "### Data Structuring and Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import folium\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from numpy import array\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "from keras.layers import TimeDistributed, RepeatVector\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from textwrap import wrap\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9189aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96023afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs = []\n",
    "summary_dfs = []\n",
    "\n",
    "# Read all csv files from directory\n",
    "# Sort files into timeseries and summary data\n",
    "for file_path in glob.glob('D:/Flood Deep Learning/**/*.csv', recursive=True):\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path, low_memory=False) \n",
    "\n",
    "    #skip these files\n",
    "    if file_name in ['streamflow_QualityCodes']:\n",
    "        continue\n",
    "\n",
    "    if 'year' in df.columns:    \n",
    "        df['source'] = file_name\n",
    "        df= df[df['year'] > 1990]\n",
    "        df= df.drop_duplicates(['year','month','day'])\n",
    "        timeseries_dfs.append(df)\n",
    "    else:\n",
    "        df = df.rename({'ID':'station_id'}, axis=1)\n",
    "        df = df.set_index('station_id')\n",
    "        summary_dfs.append(df)\n",
    "\n",
    "timeseries_data = pd.concat(timeseries_dfs, axis=0, ignore_index=True)\n",
    "timeseries_data['date'] = pd.to_datetime(timeseries_data[['year', 'month', 'day']])\n",
    "timeseries_data = timeseries_data.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "summary_data = pd.concat(summary_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c59bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self, timeseries_data=timeseries_data, summary_data=summary_data):\n",
    "        ### Data Cleaning\n",
    "        self.timeseries_data = timeseries_data.replace(-99.99,np.NaN)\n",
    "        \n",
    "        ### Feature Engineering\n",
    "        # get precipitation deficit\n",
    "        actualTransEvap_data = self.timeseries_data[self.timeseries_data['source'] == 'et_morton_actual_SILO'].drop(['source'], axis=1)\n",
    "        precipitation_data = self.timeseries_data[self.timeseries_data['source'] == 'precipitation_AWAP'].drop(['source'], axis=1)\n",
    "         \n",
    "        actualTransEvap_data = actualTransEvap_data[actualTransEvap_data['date'].isin(precipitation_data['date'])].reset_index(drop=True)\n",
    "        precipitation_data = precipitation_data[precipitation_data['date'].isin(actualTransEvap_data['date'])].reset_index(drop=True)\n",
    "        \n",
    "        self.precipitation_deficit = precipitation_data.drop(['date'], axis=1).subtract(actualTransEvap_data.drop(['date'], axis=1))\n",
    "        self.precipitation_deficit['source'] = 'precipitation_deficit'\n",
    "        self.precipitation_deficit['date'] = precipitation_data['date']\n",
    "        \n",
    "        # get flood probabilities\n",
    "        self.streamflow_data = self.timeseries_data[timeseries_data['source'] == 'streamflow_MLd_inclInfilled'].drop(['source'], axis=1)\n",
    "        self.streamflow_data = self.streamflow_data.set_index('date')\n",
    "        \n",
    "        self.flood_probabilities = self.streamflow_data.apply(self.flood_extent, axis=0)\n",
    "        self.flood_probabilities['source'] = 'flood_probabilities'\n",
    "        self.flood_probabilities['date'] = self.streamflow_data.index\n",
    "        \n",
    "        self.flood_indicator = self.flood_probabilities.applymap(lambda x: int(x <0.05) if pd.isnull(x) == False and isinstance(x, float) else x)\n",
    "        self.flood_indicator['source'] = 'flood_indicator'\n",
    "        self.flood_indicator['date'] = self.flood_probabilities['date']        \n",
    "        \n",
    "        # turn date into sin and cos function \n",
    "        date_min = np.min(self.flood_probabilities['date'])\n",
    "        year_seconds = 365.2425*24*60*60\n",
    "        year_sin = self.flood_probabilities['date'].apply(lambda x: np.sin((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        year_cos = self.flood_probabilities['date'].apply(lambda x: np.cos((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        all_stations = list(self.flood_probabilities.drop(columns=['source', 'date'], axis=1).columns) \n",
    "        \n",
    "        df_sin = []     \n",
    "        for value in year_sin:\n",
    "            df_sin.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_sin = pd.DataFrame(df_sin)\n",
    "        df_sin['source'] = 'year_sin'\n",
    "        df_sin['date'] = self.flood_probabilities['date']\n",
    " \n",
    "        df_cos = []\n",
    "        for value in year_cos:\n",
    "            df_cos.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_cos = pd.DataFrame(df_cos)\n",
    "        df_cos['source'] = 'year_cos'\n",
    "        df_cos['date'] = self.flood_probabilities['date']\n",
    "            \n",
    "        ### Return\n",
    "        self.timeseries_data = pd.concat([self.timeseries_data, self.precipitation_deficit, self.flood_probabilities, df_sin, df_cos, self.flood_indicator], axis=0).reset_index(drop=True)\n",
    "        self.summary_data = summary_data\n",
    "        \n",
    "    def get_timeseries_data(self, source, stations):      \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        return self.data_filtered\n",
    "        \n",
    "        \n",
    "    def get_data(self, source, stations):\n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]\n",
    "     \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(timeseries_source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                self.data_filtered[station, variable] = value\n",
    "        \n",
    "        return self.data_filtered.sort_index(axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_train_val_test(self, source, stations, \n",
    "                           scaled=True, target=['streamflow_MLd_inclInfilled'],\n",
    "                           start=None, end=None,\n",
    "                           discard=0.05, train=0.6, test=0.4):\n",
    "        assert 0<=discard<=1\n",
    "        assert (train + test) == 1\n",
    "     \n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]        \n",
    "        \n",
    "        all_data = self.get_timeseries_data(timeseries_source, stations).loc[start:end]\n",
    "        n_rows_all = len(all_data)\n",
    "        \n",
    "        all_data_discarded = all_data.iloc[int(n_rows_all*discard):]\n",
    "        n_rows_discarded = len(all_data_discarded)\n",
    "        \n",
    "        train_df = all_data_discarded[:int(n_rows_discarded*train)]\n",
    "        test_df = all_data_discarded[-int(n_rows_discarded*(test)):]\n",
    "        \n",
    "        if scaled == True:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_df)\n",
    "            \n",
    "            scaler_test = MinMaxScaler()\n",
    "            scaler_test.fit(test_df)\n",
    "            \n",
    "            train_df = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "            test_df = pd.DataFrame(scaler_test.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "            \n",
    "     \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                \n",
    "                train_df[station, variable] = value                \n",
    "                test_df[station, variable] = value \n",
    "                                  \n",
    "        return train_df.sort_index(axis=1), test_df.sort_index(axis=1) \n",
    "    \n",
    "    def flood_extent(self, streamflow_ts):\n",
    "        station_name = streamflow_ts.name\n",
    "\n",
    "        flow_data = pd.DataFrame(streamflow_ts)  \n",
    "        na_values = flow_data[flow_data[station_name].isna()][station_name]\n",
    "\n",
    "        flow_data = flow_data.dropna().sort_values(by=station_name, ascending=False).reset_index()\n",
    "        flow_data['probability'] = (flow_data.index + 1)/(1+len(flow_data)) \n",
    "        flow_data = flow_data.sort_values(by='date').drop(['date', station_name], axis=1)['probability']\n",
    "        flow_data = pd.concat([na_values, flow_data]).reset_index(drop=True) \n",
    "        flow_data.name = station_name  \n",
    "\n",
    "        return flow_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89db481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camels_data = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, test_df, \n",
    "                 station, filtered=None, label_columns=None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df[station]\n",
    "        self.test_df = test_df[station]\n",
    "        self.station = station\n",
    "        self.filtered = filtered\n",
    "        \n",
    "        # validation\n",
    "        if self.filtered == 'upper_soil_filter':\n",
    "            assert('upper_soil_indicator' in list(self.train_df.columns))\n",
    "        \n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(self.train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, filtered=None, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length=self.total_window_size,\n",
    "                sequence_stride=1,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        \n",
    "        if filtered == 'upper_soil_filter':\n",
    "            indicator_index = list(self.train_df.columns).index('upper_soil_indicator')\n",
    "            ds = ds.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, indicator_index]) > 0).batch(32)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df, filtered=self.filtered)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df, shuffle=False)\n",
    "           \n",
    "    @property\n",
    "    def test_windows(self):\n",
    "        total_size = self.test_array.shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    @property\n",
    "    def test_array(self):\n",
    "        return array(self.test_df[self.label_columns])\n",
    "       \n",
    "    def test_indicator(self, filtered):\n",
    "        if filtered == 'upper_soil_filter':\n",
    "            return array(self.test_df['upper_soil_indicator'])       \n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "            \n",
    "        return result\n",
    "       \n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self.train_df.shape[1]\n",
    "    \n",
    "    def test_example(self, index):\n",
    "        return np.array(self.test_df.iloc[index]).reshape(1, 1, self.num_features)\n",
    "\n",
    "    def plot(self, model=None, plot_col='streamflow_MLd_inclInfilled', max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                              marker='X', edgecolors='k', label='Predictions',\n",
    "                              c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ed6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df,test_df, stations, \n",
    "                 statics='separate', filtered=-1000, label_columns=None):\n",
    "     \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df       \n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations) \n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.filtered = filtered\n",
    "           \n",
    "        self.trains = []\n",
    "        self.tests = []        \n",
    "\n",
    "        for i, s in enumerate(stations):\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "\n",
    "            padding = np.zeros((input_width, self.total_stations), dtype=np.float32)\n",
    "            padding[:, i] = 1\n",
    "\n",
    "            self.trains.append(window.train.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            self.tests.append(window.test.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.trains)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE).filter(lambda x, y: tf.math.reduce_max(y) > self.filtered)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.tests)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])\n",
    "    \n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb25c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNumpyWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 timeseries_source, summary_source, summary_data,\n",
    "                 stations, label_columns=None):\n",
    "        \n",
    "        train_df, test_df = camels_data.get_train_val_test(source=timeseries_source,\n",
    "                                                                   stations=stations)      \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations)\n",
    "        \n",
    "        self.num_timeseries_features = len(timeseries_source)\n",
    "        self.num_static_features = len(summary_source)     \n",
    "        self.timeseries_source = timeseries_source\n",
    "        self.summary_source = summary_source      \n",
    "    \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.train_timeseries = []\n",
    "        self.test_timeseries = []  \n",
    "        \n",
    "        self.train_static = []\n",
    "        self.test_static = [] \n",
    "        \n",
    "        self.train_y = []\n",
    "        self.test_y = []\n",
    "        \n",
    "        for i, s in enumerate(stations):\n",
    "            # process timeseries\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "            \n",
    "            x_train, y_train = self.mapdata_tonumpy(window.train)\n",
    "            x_test, y_test = self.mapdata_tonumpy(window.test)\n",
    "            \n",
    "            self.train_timeseries.extend(x_train) \n",
    "            self.test_timeseries.extend(x_test) \n",
    "            \n",
    "            self.train_y.extend(y_train) \n",
    "            self.test_y.extend(y_test)      \n",
    "            \n",
    "            # process static\n",
    "            static = summary_data[summary_source].loc[s].to_numpy()         \n",
    "            padding = np.zeros((self.total_stations, ), dtype=np.float32)\n",
    "            padding[i] = 1\n",
    "            \n",
    "            static = np.concatenate([static, padding], axis=0)\n",
    "            \n",
    "            self.train_static.extend([static for _ in range(x_train.shape[0])])\n",
    "            self.test_static.extend([static for _ in range(x_test.shape[0])])\n",
    "            \n",
    "        self.train_timeseries = np.array(self.train_timeseries)\n",
    "        self.test_timeseries = np.array(self.test_timeseries) \n",
    "        \n",
    "        self.train_static = np.array(self.train_static)\n",
    "        self.test_static = np.array(self.test_static)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.train_static)\n",
    "        \n",
    "        self.train_static = scaler.transform(self.train_static)\n",
    "        self.test_static = scaler.transform(self.test_static)\n",
    "                \n",
    "        \n",
    "        #self.train_static = np.ones((self.train_static.shape[0], self.train_static.shape[1]))\n",
    "        \n",
    "        self.train_y = np.array(self.train_y) \n",
    "        self.train_y = np.swapaxes(self.train_y, 1, 2)\n",
    "\n",
    "        \n",
    "        self.test_y = np.array(self.test_y)\n",
    "        self.test_y = np.swapaxes(self.test_y, 1, 2)        \n",
    "\n",
    "    def mapdata_tonumpy(self, map_ds):\n",
    "        map_ds = map_ds.unbatch()\n",
    "\n",
    "        x_array = []\n",
    "        y_array = []\n",
    "\n",
    "        for ts in map_ds:\n",
    "            x = ts[0]\n",
    "            y= ts[1]\n",
    "\n",
    "            x_array.append(x.numpy())\n",
    "            y_array.append(y.numpy())\n",
    "\n",
    "        return np.array(x_array), np.array(y_array) \n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.train_timeseries, self.train_static, self.train_y\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):      \n",
    "        return self.test_timeseries, self.test_static, self.test_y\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40570b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss():  \n",
    "    def qloss_95(y_true, y_pred, q=0.95):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "        \n",
    "    def qloss_90(y_true, y_pred, q=0.9):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_70(y_true, y_pred, q=0.7):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "    def qloss_50(y_true, y_pred, q=0.5):\n",
    "        e = (y_true-y_pred)    \n",
    "        return tf.square(y_true-y_pred) + K.maximum(q*e, (q-1)*e)\n",
    "    \n",
    "class Model():\n",
    "    MAX_EPOCHS = 150\n",
    "    \n",
    "    def __init__(self, window):\n",
    "        # Store the raw data.\n",
    "        self.window = window\n",
    "        \n",
    "        self.train_df = self.window.train_df\n",
    "        self.test_df = self.window.test_df\n",
    "             \n",
    "    def compile_and_fit(self, model, window, loss_func, patience=10):\n",
    "\n",
    "        model.compile(loss=loss_func,\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = model.fit(window.train, epochs=self.MAX_EPOCHS,\n",
    "                            verbose=0)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def num_flood_events(self, cut=1):\n",
    "        actuals = np.squeeze(self.window.test_windows, axis=2) \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "        locs = np.unique(np.where(actuals<cut_percentile)[0])\n",
    "\n",
    "        events = np.split(locs, np.cumsum( np.where(locs[1:] - locs[:-1] > 1) )+1)\n",
    "\n",
    "        return len(events)\n",
    "    \n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['model_name'] = self.model_name\n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "             \n",
    "        if station == None:\n",
    "            summary_dict['station'] = self.window.station\n",
    "            summary_dict['inputs'] = str(list(self.train_df.columns))\n",
    "            summary_dict['NSE'] = self.get_NSE()\n",
    "        else:\n",
    "            summary_dict['station'] = station\n",
    "            example_station = self.train_df.columns.get_level_values(0)[0]\n",
    "            summary_dict['inputs'] = str(list(self.train_df[example_station].columns))\n",
    "            summary_dict['NSE'] = self.get_NSE(station)  \n",
    "                             \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "\n",
    "        summary_dict['f1_score_individual_1%'] = self.binary_metrics(station=station, cut=1, metric='f1_score', evaluation='individual')        \n",
    "        summary_dict['f1_score_individual_2%'] = self.binary_metrics(station=station, cut=2, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_5%'] = self.binary_metrics(station=station, cut=5, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_10%'] = self.binary_metrics(station=station, cut=10, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_25%'] = self.binary_metrics(station=station, cut=25, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_50%'] = self.binary_metrics(station=station, cut=50, metric='f1_score', evaluation='individual')  \n",
    "        summary_dict['f1_score_individual_75%'] = self.binary_metrics(station=station, cut=75, metric='f1_score', evaluation='individual') \n",
    "        summary_dict['f1_score_individual_all'] = self.binary_metrics(station=station, cut=100, metric='f1_score', evaluation='individual') \n",
    "          \n",
    "        return summary_dict\n",
    "            \n",
    "    def print_model_error(self, station=None, cut=0):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "            test_array = self.window.test_array(station)\n",
    "        else:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows  \n",
    "            test_array = self.window.test_array\n",
    "            \n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.unique(np.where(actuals>cut_percentile)[0])\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        for window_pred, window_actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(test_array[loc:loc+self.window.input_width].flatten()))\n",
    "            print(\"Predicted: {}\".format(window_pred))\n",
    "            print(\"Actual: {}\".format(window_actual))\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "    def model_predictions_less_than_cut(self, cut=100):\n",
    "        \n",
    "        preds = self.predictions\n",
    "        actuals =self.window.test_windows\n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        num_predicted = (preds.flatten() < cut_percentile).sum()\n",
    "        num_actual = (actuals.flatten() < cut_percentile).sum()\n",
    "\n",
    "        return num_predicted, num_actual\n",
    "        \n",
    "    def average_model_error(self, station=None, cut=100):\n",
    "        if self.window.label_columns[0] == 'streamflow_MLd_inclInfilled':\n",
    "            cut = 100 - cut\n",
    "            \n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows         \n",
    "\n",
    "        cut_percentile = np.percentile(actuals.flatten(), cut)\n",
    "\n",
    "        locs = np.where(actuals>cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "\n",
    "        return avg_error\n",
    "    \n",
    "    def get_NSE(self, station=None, type='cast'):\n",
    "        if station != None:\n",
    "            preds = self.predictions(station)\n",
    "            actuals = self.window.test_windows(station)\n",
    "        else:\n",
    "            preds = self.predictions()\n",
    "            actuals = self.window.test_windows\n",
    "        \n",
    "        NSE = []\n",
    "\n",
    "        for i in range(self.window.label_width):\n",
    "            numer = np.sum(np.square(preds[:, i] - actuals[:, i]))\n",
    "            denom = np.sum(np.square(actuals[:, i] - np.mean(actuals[:, i])))\n",
    "        \n",
    "            NSE.append(1-(numer/denom))\n",
    "        \n",
    "        if type=='cast':\n",
    "            return np.mean(NSE)\n",
    "        else:\n",
    "            return NSE\n",
    "\n",
    "    def binary_metrics(self, cut, metric, evaluation='whole', station=None):\n",
    "        percentile_cut = self.window.station_percentile(station=station, cut=cut)\n",
    "        \n",
    "        if station==None:\n",
    "            preds_pre = self.predictions()\n",
    "            actuals_pre = self.window.test_windows\n",
    "        else:        \n",
    "            preds_pre = self.predictions(station)\n",
    "            actuals_pre = self.window.test_windows(station)\n",
    "            \n",
    "        if evaluation=='whole':  \n",
    "            preds = np.array([int(any(x > percentile_cut)) for x in preds_pre])\n",
    "            actuals = np.array([int(any(x > percentile_cut)) for x in actuals_pre])\n",
    "        else:\n",
    "            preds = np.array([int(x > percentile_cut) for x in preds_pre.flatten()])           \n",
    "            actuals = np.array([int(x > percentile_cut) for x in actuals_pre.flatten()])\n",
    "\n",
    "        if metric=='accuracy':\n",
    "            return accuracy_score(actuals, preds)\n",
    "        elif metric=='precision':\n",
    "            return precision_score(actuals, preds)\n",
    "        elif metric=='recall':\n",
    "            return recall_score(actuals, preds)\n",
    "        elif metric=='f1_score':\n",
    "            return f1_score(actuals, preds)\n",
    "     \n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "\n",
    "    def predictions(self, station=None):\n",
    "        tf_test = self.window.test\n",
    "\n",
    "        if station != None:\n",
    "            filter_index = self.window.stations.index(station)\n",
    "            num_inputs = len(self.window.train_df.columns.levels[1])\n",
    "            tf_test = tf_test.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, num_inputs + filter_index]) > 0).batch(32)\n",
    "\n",
    "        return np.squeeze(self.model.predict(tf_test), axis=2)\n",
    "        \n",
    "class Base_Model(Model):  \n",
    "    def __init__(self, model_name, window, CONV_WIDTH, output_activation='sigmoid', loss_func=tf.losses.MeanSquaredError()):\n",
    "        super().__init__(window)\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.mix_type_name = None\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        if self.model_name == 'multi-linear':          \n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Take the last time step.\n",
    "                            # Shape [batch, time, features] => [batch, 1, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "                            # Shape => [batch, 1, dense_units]\n",
    "                            tf.keras.layers.Dense(20, activation='relu'),\n",
    "                            # Shape => [batch, out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-CNN':\n",
    "            self.model = tf.keras.Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "                            tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "                            # Shape => [batch, 1, conv_units]\n",
    "                            tf.keras.layers.Conv1D(64, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "                            # Shape => [batch, 1,  out_steps*features]\n",
    "                            tf.keras.layers.Dense(CONV_WIDTH, activation=output_activation, \n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1]\n",
    "                            tf.keras.layers.Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "            \n",
    "        elif self.model_name == 'multi-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=False),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])\n",
    "\n",
    "            \n",
    "        elif self.model_name == 'multi-ED-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            LSTM(20, return_sequences=True,),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dropout(0.2),\n",
    "                            Flatten(),\n",
    "                            RepeatVector(5),\n",
    "                            LSTM(20, return_sequences=False), \n",
    "                            \n",
    "                            Dropout(0.2),                \n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])    \n",
    "            \n",
    "        elif self.model_name == 'multi-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(20, return_sequences=False)),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ])  \n",
    "            \n",
    "        elif self.model_name == 'multi-deep-Bidirectional-LSTM':                       \n",
    "            self.model = Sequential([\n",
    "                            # Shape [batch, time, features] => [batch, lstm_units].\n",
    "                            # Adding more `lstm_units` just overfits more quickly.\n",
    "                            Bidirectional(LSTM(64, return_sequences=True\n",
    "                                              )),\n",
    "                            Dropout(0.2),\n",
    "                            Bidirectional(LSTM(32, return_sequences=False)),\n",
    "                            Dropout(0.2),\n",
    "                            # Shape => [batch, out_steps*features].\n",
    "                            Dense(CONV_WIDTH, activation=output_activation,\n",
    "                                                  kernel_initializer=tf.initializers.zeros()),\n",
    "                            # Shape => [batch, out_steps, features=1].\n",
    "                            Reshape([CONV_WIDTH, 1])\n",
    "                        ]) \n",
    "            \n",
    "        self.compile_and_fit(self.model, window, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82744dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixed_Model(Base_Model):\n",
    "    threshold = 0.2\n",
    "    \n",
    "    def __init__(self, model_name, mix_type_name, window, CONV_WIDTH):\n",
    "        super().__init__(model_name, window, CONV_WIDTH)\n",
    "        self.mix_type_name = mix_type_name\n",
    "               \n",
    "        if self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=1,\n",
    "                                             shift=1,\n",
    "                                             train_df=train_df.loc[:,train_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             test_df=test_df.loc[:,test_df.columns.get_level_values(1).isin(self.window.label_columns)] ,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=1)\n",
    "            \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=1,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            window_simple = WindowGenerator(input_width=self.window.input_width,\n",
    "                                             label_width=self.window.label_width,\n",
    "                                             shift=self.window.label_width,\n",
    "                                             train_df=train_df,\n",
    "                                             test_df=test_df,\n",
    "                                             station=self.window.station,\n",
    "                                            filtered='upper_soil_filter',\n",
    "                                             label_columns=['flood_probabilities'])\n",
    "            \n",
    "            self.model_simple = Base_Model(model_name=model_name, window=window_simple, CONV_WIDTH=self.window.label_width)\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        if self.mix_type_name == 'simple':\n",
    "            preds = super().predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            new_pred=[]\n",
    "       \n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = np.full((self.window.label_width,), actual_before)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-onestepAR':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for pred, actual_before in zip(preds, test_array):\n",
    "                if actual_before < self.threshold:\n",
    "                    pred = []\n",
    "                                      \n",
    "                    input_value = np.array(actual_before).reshape(1,1,1)\n",
    "                    \n",
    "                    for j in range(self.window.label_width):\n",
    "                        pred_simple = self.model_simple.model.predict(input_value).item()\n",
    "                        pred.append(pred_simple)\n",
    "                        \n",
    "                        input_value = np.array(pred_simple).reshape(1,1,1)\n",
    "                                         \n",
    "                    pred = np.array(pred)\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'simple-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # test array starts 1 time unit before predictions\n",
    "            test_array = self.window.test_array[self.window.input_width:]\n",
    "            \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, actual_before) in enumerate(zip(preds, test_array)):\n",
    "                if actual_before < self.threshold:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)  \n",
    "            \n",
    "            return np.array(new_pred)\n",
    "        \n",
    "        elif self.mix_type_name == 'upper_soil-two_model-multistep':\n",
    "            preds = super().predictions\n",
    "            preds_simple = self.model_simple.predictions\n",
    "            \n",
    "            # upper soil indicator 1 time unit before predictions\n",
    "            upper_soil_indicator = window.test_indicator(filtered='upper_soil_filter')\n",
    "                      \n",
    "            new_pred=[]\n",
    "\n",
    "            for i, (pred, indicator) in enumerate(zip(preds, upper_soil_indicator)):\n",
    "                if indicator == 1:                                \n",
    "                    input_value = self.window.test_example(i+self.window.input_width)\n",
    "                                              \n",
    "                    pred = self.model_simple.model.predict(input_value).flatten()\n",
    "\n",
    "                new_pred.append(pred)              \n",
    "            \n",
    "            return np.array(new_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5344ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without Changes\n",
    "\n",
    "class Ensemble_Static():\n",
    "    epochs = 100\n",
    "    patience = 5\n",
    "    def __init__(self, numpy_window, batch_size=32):\n",
    "        num_timesteps = numpy_window.input_width\n",
    "        num_timeseries_features = numpy_window.num_timeseries_features\n",
    "        num_static_features = numpy_window.num_static_features + numpy_window.total_stations\n",
    "          \n",
    "        num_predictions = numpy_window.label_width\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.stations = numpy_window.stations\n",
    "        self.n_stations = numpy_window.total_stations\n",
    "        self.numpy_window = numpy_window\n",
    "        # RNN + SLP Model\n",
    "        # Define input layer\n",
    "\n",
    "        recurrent_input = Input(shape=(num_timesteps, num_timeseries_features),name=\"TIMESERIES_INPUT\")\n",
    "        static_input = Input(shape=(num_static_features,),name=\"STATIC_INPUT\")\n",
    "\n",
    "        # RNN Layers\n",
    "        # layer - 1\n",
    "        rec_layer_one = LSTM(20, name =\"BIDIRECTIONAL_LAYER_1\", return_sequences=True)(recurrent_input)\n",
    "        rec_layer_one = Dropout(0.1,name =\"DROPOUT_LAYER_1\")(rec_layer_one)\n",
    "        \n",
    "        # layer - 2\n",
    "        rec_layer_two = LSTM(20, name =\"BIDIRECTIONAL_LAYER_2\", return_sequences=False)(rec_layer_one)\n",
    "        rec_layer_two = Dropout(0.1,name =\"DROPOUT_LAYER_2\")(rec_layer_two)      \n",
    "        \n",
    "\n",
    "        # SLP Layers\n",
    "        static_layer_one = Dense(20, activation='relu',name=\"DENSE_LAYER_1\")(static_input)\n",
    "        # Combine layers - RNN + SLP\n",
    "        combined = Concatenate(axis= 1,name = \"CONCATENATED_TIMESERIES_STATIC\")([rec_layer_two, static_layer_one])\n",
    "        combined_dense_two = Dense(20, activation='relu',name=\"DENSE_LAYER_2\")(combined)\n",
    "        output = Dense(num_predictions, name=\"OUTPUT_LAYER\", activation='sigmoid')(combined_dense_two)\n",
    "\n",
    "      \n",
    "        # Compile ModeL\n",
    "        self.model = keras.models.Model(inputs=[recurrent_input, static_input], outputs=[output])\n",
    "        # MSE\n",
    "        \n",
    "        #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self.train_timeseries_x, self.train_static_x, self.train_y = numpy_window.train     \n",
    "        self.test_timeseries_x, self.test_static_x, self.test_y = numpy_window.test \n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.compile(loss='MeanSquaredError', optimizer='adam', metrics=['MeanAbsoluteError'])\n",
    "        \n",
    "        \n",
    "        self.model.fit([self.train_timeseries_x, self.train_static_x], \n",
    "                       self.train_y, \n",
    "                       epochs=self.epochs, \n",
    "                       batch_size=self.batch_size, \n",
    "                       verbose=1)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test_loss(self):\n",
    "        return self.model.evaluate(self.window.test, verbose=0)[0]\n",
    "    \n",
    "    def predictions(self, station):  \n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        \n",
    "        print(\"Timeseries input shape:\", self.test_timeseries_x[start:end].shape)\n",
    "        print(\"Static input shape:\", self.test_static_x[start:end].shape)\n",
    "        \n",
    "        return self.model.predict([self.test_timeseries_x[start:end,:], self.test_static_x[start:end,:]])\n",
    "    \n",
    "    \n",
    "    def actuals(self, station):\n",
    "        filter_index = self.stations.index(station)\n",
    "        n_observations = int(self.test_static_x.shape[0]/self.n_stations)\n",
    "\n",
    "        start = int(filter_index*n_observations)\n",
    "        end = int((filter_index+1)*n_observations)\n",
    "        print('TSET Y SHAPE',self.test_y.shape)\n",
    "        \n",
    "\n",
    "        return self.test_y.reshape(self.test_y.shape[0], self.test_y.shape[2])[start:end, :]\n",
    "\n",
    "    \n",
    "    def average_model_error(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        \n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "\n",
    "        avg_error = 0\n",
    "\n",
    "        for window_pred, window_actual in zip(preds, actuals):\n",
    "            avg_error += np.sum((window_pred - window_actual)**2)\n",
    "        \n",
    "        if avg_error==0:\n",
    "            return 0\n",
    "        print(avg_error)\n",
    "        avg_error = avg_error/actuals.shape[0]*actuals.shape[1]\n",
    "\n",
    "        return avg_error \n",
    "    \n",
    "    def print_model_windows(self, station, cut=100):\n",
    "        preds = self.predictions(station)\n",
    "        actuals = self.actuals(station)\n",
    "        cut_percentile = np.percentile(actuals.flatten(), 100-cut)\n",
    "\n",
    "        locs = np.where(actuals > cut_percentile)[0]\n",
    "        preds = preds[locs]\n",
    "        actuals = actuals[locs]\n",
    "        \n",
    "        for pred, actual, loc in zip(preds, actuals, locs):\n",
    "            print(\"time: {}\".format(loc))\n",
    "            print(\"Input: {}\".format(self.test_y[loc:loc+self.numpy_window.input_width+1].flatten()))\n",
    "            print(\"Predicted: {}\".format(pred))\n",
    "            print(\"Actual: {}\".format(actual))\n",
    "            print(\"-------------------------\")        \n",
    "\n",
    "    def summary(self, station):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "        summary_dict['input_width'] = self.numpy_window.input_width\n",
    "        summary_dict['label_width'] = self.numpy_window.label_width\n",
    "        summary_dict['num_timeseries_features'] = self.numpy_window.num_timeseries_features \n",
    "        summary_dict['num_static_features'] = self.numpy_window.num_static_features        \n",
    "        summary_dict['timeseries_inputs'] = self.numpy_window.timeseries_source\n",
    "        summary_dict['static_inputs'] = self.numpy_window.summary_source     \n",
    "\n",
    "   \n",
    "        summary_dict['SERA_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SERA_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SERA_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SERA_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SERA_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SERA_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SERA_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['SERA_all'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea442a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Switch_Model(Model):\n",
    "    threshold = 0.7\n",
    "    \n",
    "    def __init__(self, window_switch, window_regular, CONV_WIDTH):\n",
    "        self.window_switch = window_switch\n",
    "        self.window = window_regular\n",
    "        \n",
    "        assert(window_switch.input_width == self.window.input_width)\n",
    "        \n",
    "        self.switch = Ensemble_Static(window_switch)\n",
    "        \n",
    "        self.regular = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH)\n",
    "        self.q70 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_70)\n",
    "        self.q95 = Base_Model(model_name='multi-LSTM', window=window_regular, CONV_WIDTH=CONV_WIDTH, loss_func=CustomLoss.qloss_95)\n",
    "        \n",
    "    def predictions(self, station):\n",
    "        preds_switch = self.switch.predictions(station)     \n",
    "        \n",
    "        preds_regular = self.regular.predictions(station)\n",
    "        preds_q70 = self.q70.predictions(station)        \n",
    "        preds_q95 = self.q95.predictions(station)\n",
    "        \n",
    "        test_array = self.window.test_windows(station)   \n",
    "\n",
    "        new_pred=[]\n",
    "        \n",
    "        for pred_switch, pred_regular, pred_q70, pred_q95 in zip(preds_switch, preds_regular, preds_q70, preds_q95):\n",
    "\n",
    "                \n",
    "            switch_condition = pred_switch > 0.95\n",
    "            q95_condition = pred_switch > 0.7\n",
    "            q70_condition = pred_switch <= 0.7  # You might want to specify this condition differently\n",
    "\n",
    "            new_pred.append(np.where(switch_condition, pred_q95, np.where(q95_condition, pred_q70, pred_regular)))\n",
    "                \n",
    "        return np.array(new_pred)\n",
    "        \n",
    "\n",
    "    def test_MSE(self, station=None):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = self.window.test_array(station)[self.window.input_width:]\n",
    "\n",
    "        return mean_squared_error(test_array, preds)\n",
    "    \n",
    "    def test_ROCAUC(self, station, level=0.05):\n",
    "        preds = self.predictions(data='test', station=station)\n",
    "        test_array = (self.window.test_array(station)[self.window.input_width:] < level).astype(int)\n",
    "        \n",
    "        return roc_auc_score(test_array, preds)\n",
    "\n",
    "    def summary(self, station=None):\n",
    "        summary_dict = {}\n",
    "        \n",
    "        summary_dict['input_width'] = self.window.input_width\n",
    "        summary_dict['label_width'] = self.window.label_width\n",
    "        \n",
    "        summary_dict['station'] = station\n",
    "\n",
    "        summary_dict['NSE'] = self.get_NSE(station)       \n",
    "                  \n",
    "        summary_dict['SER_1%'] = self.average_model_error(station, cut=1)\n",
    "        summary_dict['SER_2%'] = self.average_model_error(station, cut=2)    \n",
    "        summary_dict['SER_5%'] = self.average_model_error(station, cut=5)        \n",
    "        summary_dict['SER_10%'] = self.average_model_error(station, cut=10)  \n",
    "        summary_dict['SER_25%'] = self.average_model_error(station, cut=25)  \n",
    "        summary_dict['SER_50%'] = self.average_model_error(station, cut=50)  \n",
    "        summary_dict['SER_75%'] = self.average_model_error(station, cut=75)  \n",
    "        summary_dict['RMSE'] = self.average_model_error(station, cut=100)\n",
    "        \n",
    "        return summary_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e435a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_data.summary_data= camels_data.summary_data.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dcca3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIC    72\n",
       "QLD    61\n",
       "NSW    33\n",
       "NT     16\n",
       "WA     16\n",
       "TAS    12\n",
       "SA      9\n",
       "ACT     3\n",
       "Name: state_outlet, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_data.summary_data['state_outlet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79212b",
   "metadata": {},
   "source": [
    "### Plotting catchments on Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66324381",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_sd = camels_data.summary_data.loc[:,~camels_data.summary_data.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the list of cities and their latitudes/longitudes\n",
    "cities = cd_sd['station_name']\n",
    "lats = cd_sd['lat_outlet']\n",
    "longs = cd_sd['long_outlet']\n",
    "\n",
    "# Generate a random priority for each city between 1 and 5\n",
    "priority = np.random.randint(1, 6, size=len(cities))\n",
    "state= cd_sd['state_outlet']\n",
    "\n",
    "# Create the DataFrame with the city data\n",
    "data = {'cityname': cities,\n",
    "        'lats': lats,\n",
    "        'longs': longs,\n",
    "        'States': state,\n",
    "        'priority': priority\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b64dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {'QLD': 1, 'NSW': 2, 'SA': 3, 'VIC': 4, 'ACT': 5, 'WA': 6, 'NT': 7, 'TAS': 8}\n",
    "df['state_num'] = df['States'].map(state_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile of Australia\n",
    "australia = gpd.read_file('STE_2021_AUST_SHP_GDA2020/STE_2021_AUST_GDA2020.shp')\n",
    "\n",
    "# Define the CRS of the shapefile manually\n",
    "australia.crs = 'epsg:7844'\n",
    "\n",
    "# Create a GeoDataFrame from the DataFrame of cities\n",
    "gdf_cities = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longs, df.lats))\n",
    "\n",
    "# Set the CRS of the GeoDataFrame to EPSG 7844\n",
    "# https://epsg.io/7844\n",
    "gdf_cities.crs = 'epsg:7844'\n",
    "\n",
    "# Reproject the GeoDataFrame of cities to match the CRS of the shapefile\n",
    "gdf_cities = gdf_cities.to_crs(australia.crs)\n",
    "\n",
    "# Perform a spatial join to link the cities to their corresponding polygons in the shapefile\n",
    "gdf_cities = gpd.sjoin(gdf_cities, australia, predicate='within')\n",
    "\n",
    "# Set up the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# # Define a custom dark color palette\n",
    "custom_palette = sns.color_palette(['darkblue', 'black', 'purple','darkred', 'darkgreen', 'darkorange', 'brown' , 'blue'], n_colors=len(df['state_num'].unique()))\n",
    "\n",
    "# Plot the cities colored by priority with adjustments\n",
    "sns.scatterplot(ax=ax, data=gdf_cities, x='longs', y='lats', hue='States', s=15, palette=custom_palette, edgecolor='black', alpha=0.8, legend='full', zorder=2)\n",
    "\n",
    "\n",
    "# Set x-axis limits\n",
    "ax.set_xlim(110, 160)\n",
    "\n",
    "# Add the shapefile of Australia as a background map\n",
    "australia.plot(ax=ax, color='lightgrey', edgecolor='white', zorder=1)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Catchments across Australia')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec923ee5",
   "metadata": {},
   "source": [
    "### Stage- 3 Quantile LSTM Model for All States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "497f7390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIC    72\n",
       "QLD    61\n",
       "NSW    33\n",
       "NT     16\n",
       "WA     16\n",
       "TAS    12\n",
       "SA      9\n",
       "ACT     3\n",
       "Name: state_outlet, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_data.summary_data['state_outlet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84dbb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the state you wish to run the quantile lstm model on\n",
    "selected_stations = list(camels_data.summary_data[camels_data.summary_data['state_outlet'] == 'SA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#, 'upper_soil', 'deep_soil'\n",
    "combined=[]\n",
    "for i in range(0,30):\n",
    "    print('RUN',i)\n",
    "    results_switch=[]\n",
    "    variable_ts = ['streamflow_MLd_inclInfilled', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP']\n",
    "    variable_ts_switch = ['flood_probabilities', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP']\n",
    "\n",
    "    variable_static = ['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq', 'high_q_dur', 'low_q_freq', 'zero_q_freq']\n",
    "\n",
    "    train_df, test_df = camels_data.get_train_val_test(source=variable_ts, stations=selected_stations)\n",
    "\n",
    "    multi_window = MultiWindow(input_width=5,\n",
    "                               label_width=5,\n",
    "                               shift=5,\n",
    "                               train_df=train_df,\n",
    "                               test_df=test_df,\n",
    "                               stations=selected_stations,\n",
    "                               label_columns=['streamflow_MLd_inclInfilled'])\n",
    "\n",
    "    np_window = MultiNumpyWindow(input_width=5, \n",
    "                                 label_width=5,\n",
    "                                 shift=5,\n",
    "                                 timeseries_source=variable_ts_switch,\n",
    "                                 summary_source=variable_static,\n",
    "                                 summary_data=camels_data.summary_data,\n",
    "                                 stations=selected_stations,\n",
    "                                 label_columns=['flood_probabilities'])\n",
    "\n",
    "    model_switch = Switch_Model(window_switch=np_window, window_regular=multi_window, CONV_WIDTH=5) \n",
    "\n",
    "    for station in selected_stations:\n",
    "                results_switch.append(model_switch.summary(station))\n",
    "    \n",
    "    Switch_SA= pd.DataFrame(results_switch)\n",
    "    Switch_SA= Switch_SA.mean()\n",
    "    Switch_SA= Switch_SA.to_dict()\n",
    "    combined.append(Switch_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc169dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Switch3_SA_bdlstm = pd.DataFrame(combined)\n",
    "Switch3_SA_bdlstm.to_csv('Quantilelstm_SA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d06ed",
   "metadata": {},
   "source": [
    "### Visualizations of Quantile LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdecb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get visualizations of predicted vs actual for quantile lstm model\n",
    "# Enter the station name and the horizons as per need \n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.title('SA-A5030502', fontsize= 20)\n",
    "# plt.xlabel('')\n",
    "plt.ylabel('Flood Probability', fontsize=18)\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# plt.set_xlabel(fontsize=15)\n",
    "# plt.set_ylabel('Flood Probability', fontsize=15)\n",
    "\n",
    "df_date= np_window.test_df['A5030502'].reset_index()\n",
    "date_values= df_date['date']\n",
    "\n",
    "s=800\n",
    "e=1600\n",
    "\n",
    "ax1 = plt.plot(date_values[s:e], model_switch.predictions('A5030502')[:,0][s:e], color='blue')\n",
    "ax2 = plt.plot(date_values[s:e], multi_window.test_windows('A5030502')[:,0][s:e], color='red')\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='Actual')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Predicted')\n",
    "\n",
    "plt.legend(handles=[red_patch, blue_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db6835",
   "metadata": {},
   "source": [
    "### Stage 2- Individual Architecture- SA - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008af8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations = list(camels_data.summary_data[camels_data.summary_data['state_outlet'] == 'SA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the model you wish to run or multiple as per requirements. The models can be accessed through the following names:\n",
    "#['multi-LSTM', 'multi-linear','multi-CNN', 'multi-Bidirectional-LSTM']\n",
    "\n",
    "combined= []\n",
    "for i in range(0,30):\n",
    "    print('RUN', i)\n",
    "    input_widths = [5]\n",
    "    label_widths = [5]\n",
    "    models = ['multi-LSTM']\n",
    "    variables = [['streamflow_MLd_inclInfilled', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP']]\n",
    "\n",
    "    permutations_base = list(itertools.product(*[input_widths, label_widths, selected_stations, models, variables]))\n",
    "\n",
    "    results_baseModels_variables = []\n",
    "    models_baseModels_variables = []\n",
    "    errors_baseModels_variables = []\n",
    "\n",
    "    for input_width, label_width, station, model_name, variable in permutations_base:\n",
    "        if input_width < label_width:\n",
    "            continue\n",
    "\n",
    "        train_df, test_df = camels_data.get_train_val_test(source=variable, stations=selected_stations)\n",
    "\n",
    "        try:\n",
    "            print('input_width:{}, label_width:{}, station:{}, model:{}, variables:{}'.format(input_width, label_width, station, model_name, variable))\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)\n",
    "\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=label_width,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=station,\n",
    "                                     label_columns=['streamflow_MLd_inclInfilled'])\n",
    "\n",
    "            model = Base_Model(model_name=model_name, window=window, CONV_WIDTH=label_width)\n",
    "\n",
    "            results_baseModels_variables.append(model.summary())\n",
    "\n",
    "            pd.DataFrame(results_baseModels_variables).to_csv('results_files/results_ensemble_all_1.csv')\n",
    "\n",
    "        except:\n",
    "            errors_baseModels_variables.append([input_width, label_width, station, model])\n",
    "\n",
    "    Individual_SA= pd.DataFrame(results_baseModels_variables)\n",
    "    Individual_SA= Individual_SA.mean()\n",
    "    Individual_SA= Individual_SA.to_dict()\n",
    "    combined.append(Individual_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09153b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Individual_SA_lstm = pd.DataFrame(combined)\n",
    "Individual_SA_lstm.to_csv('Individual_SA_lstm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919711aa",
   "metadata": {},
   "source": [
    "### Stage 1 - Comparing Architectures for the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec778b",
   "metadata": {},
   "source": [
    "#### Batch-Indicator LSTM- SA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "917122f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations = list(camels_data.summary_data[camels_data.summary_data['state_outlet'] == 'SA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a33abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df257e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined= []\n",
    "for i in range(0,30):\n",
    "    print('RUN',i)\n",
    "    input_widths = [5]\n",
    "    label_widths = [5]\n",
    "    models = ['multi-LSTM']\n",
    "    variables = [['streamflow_MLd_inclInfilled', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP']]\n",
    "\n",
    "    permutations_base_batch = list(itertools.product(*[input_widths, label_widths, models, variables]))\n",
    "\n",
    "    results_baseModels_batch = []\n",
    "    errors_baseModels_batch = []\n",
    "\n",
    "    for input_width, label_width, model_name, variable in permutations_base_batch:\n",
    "        try:\n",
    "            if input_width < label_width:\n",
    "                continue\n",
    "\n",
    "            train_df, test_df = camels_data.get_train_val_test(source=variable, stations=selected_stations, discard=0.5)\n",
    "            multi_window = MultiWindow(input_width=input_width,\n",
    "                                       label_width=label_width,\n",
    "                                       shift=label_width,\n",
    "                                       train_df=train_df,\n",
    "                                       test_df=test_df,\n",
    "                                       stations=selected_stations,\n",
    "                                       label_columns=['streamflow_MLd_inclInfilled'])\n",
    "\n",
    "            model = Base_Model(model_name=model_name, window=multi_window, CONV_WIDTH=label_width)\n",
    "\n",
    "            print('input_width:{}, label_width:{}, model:{}, variables:{}'.format(input_width, label_width, model_name, variable))\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)\n",
    "\n",
    "            for station in selected_stations:\n",
    "                results_baseModels_batch.append(model.summary(station=station))\n",
    "\n",
    "            pd.DataFrame(results_baseModels_batch).to_csv('results_files/results_batch_all_1.csv')\n",
    "        except:\n",
    "            errors_baseModels_batch.append([input_width, label_width, model_name]) \n",
    "    Batch_SA= pd.DataFrame(results_baseModels_batch)\n",
    "    Batch_SA= Batch_SA.mean()\n",
    "    Batch_SA= Batch_SA.to_dict()\n",
    "    combined.append(Batch_SA)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae5b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_SA_LSTM = pd.DataFrame(combined)\n",
    "Batch_SA_LSTM.to_csv('Batch_SA_LSTM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e49d7",
   "metadata": {},
   "source": [
    "### Batch-Static LSTM SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e56e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations = list(camels_data.summary_data[camels_data.summary_data['state_outlet'] == 'SA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28b483df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80705be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=[]\n",
    "for i in range(0,30):\n",
    "    print('RUN', i)\n",
    "    input_widths = [5]\n",
    "    label_widths = [5]\n",
    "    models = ['multi-LSTM']\n",
    "    variables = [['streamflow_MLd_inclInfilled', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP', 'q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq', 'high_q_dur', 'low_q_freq', 'zero_q_freq']]\n",
    "\n",
    "    permutations_base_batch = list(itertools.product(*[input_widths, label_widths, models, variables]))\n",
    "\n",
    "    results_baseModels_batch = []\n",
    "    errors_baseModels_batch = []\n",
    "\n",
    "    for input_width, label_width, model_name, variable in permutations_base_batch:\n",
    "        try:\n",
    "            if input_width < label_width:\n",
    "                continue\n",
    "\n",
    "            train_df, test_df = camels_data.get_train_val_test(source=variable, stations=selected_stations)\n",
    "            multi_window = MultiWindow(input_width=input_width,\n",
    "                                       label_width=label_width,\n",
    "                                       shift=label_width,\n",
    "                                       train_df=train_df,\n",
    "                                       test_df=test_df,\n",
    "                                       stations=selected_stations,\n",
    "                                       label_columns=['streamflow_MLd_inclInfilled'])\n",
    "\n",
    "            model = Base_Model(model_name=model_name, window=multi_window, CONV_WIDTH=label_width)\n",
    "\n",
    "            print('input_width:{}, label_width:{}, model:{}, variables:{}'.format(input_width, label_width, model_name, variable))\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(current_time)\n",
    "\n",
    "            for station in selected_stations:\n",
    "                results_baseModels_batch.append(model.summary(station=station))\n",
    "\n",
    "            pd.DataFrame(results_baseModels_batch).to_csv('results_files/results_batch_static_all_1.csv')\n",
    "        except:\n",
    "            errors_baseModels_batch.append([input_width, label_width, model])\n",
    "    Batch_SA= pd.DataFrame(results_baseModels_batch)\n",
    "    Batch_SA= Batch_SA.mean()\n",
    "    Batch_SA= Batch_SA.to_dict()\n",
    "    combined.append(Batch_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd8817ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_SA_LSTM = pd.DataFrame(combined)\n",
    "Batch_SA_LSTM.to_csv('BatchStatic_SA_LSTM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968655a",
   "metadata": {},
   "source": [
    "#### Ensemble Model LSTM- SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20ae1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations = list(camels_data.summary_data[camels_data.summary_data['state_outlet'] == 'SA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973f0b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad272cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=[]\n",
    "for i in range(0,30):\n",
    "    print('RUN:', i)\n",
    "    results_ensemble = []\n",
    "    input_widths = [5]\n",
    "    label_widths = [5]\n",
    "\n",
    "    permutations_ensemble = list(itertools.product(*[input_widths, label_widths]))\n",
    "\n",
    "    for input_width, label_width in permutations_ensemble:\n",
    "        np_window = MultiNumpyWindow(input_width=input_width, \n",
    "                                     label_width=label_width,\n",
    "                                     shift=label_width,\n",
    "                                     timeseries_source=['streamflow_MLd_inclInfilled', 'precipitation_deficit', 'year_sin', 'year_cos', 'tmax_AWAP', 'tmin_AWAP'],\n",
    "                                     summary_source=['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq', 'high_q_dur', 'low_q_freq', 'zero_q_freq'],\n",
    "                                     summary_data=camels_data.summary_data,\n",
    "                                     stations=selected_stations,\n",
    "                                     label_columns=['streamflow_MLd_inclInfilled'])\n",
    "\n",
    "        ensemble_model = Ensemble_Static(np_window)\n",
    "        ensemble_model.train()\n",
    "        print('done')\n",
    "\n",
    "        for station in selected_stations:\n",
    "            results_ensemble.append(ensemble_model.summary(station))\n",
    "                \n",
    "    Ensemble_SA= pd.DataFrame(results_ensemble)\n",
    "    Ensemble_SA= Ensemble_SA.mean()\n",
    "    Ensemble_SA= Ensemble_SA.to_dict()\n",
    "    combined.append(Ensemble_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62be2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble_SA_LSTM = pd.DataFrame(combined)\n",
    "Ensemble_SA_LSTM.to_csv('Ensemble_SA_LSTM.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
