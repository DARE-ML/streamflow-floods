{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import folium\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from numpy import array\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "from keras.layers import TimeDistributed, RepeatVector\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from textwrap import wrap\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9189aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96023afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs = []\n",
    "summary_dfs = []\n",
    "\n",
    "# Read all csv files from directory\n",
    "# Sort files into timeseries and summary data\n",
    "for file_path in glob.glob('D:/Flood Deep Learning/**/*.csv', recursive=True):\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path, low_memory=False) \n",
    "\n",
    "    #skip these files\n",
    "    if file_name in ['streamflow_QualityCodes']:\n",
    "        continue\n",
    "\n",
    "    if 'year' in df.columns:    \n",
    "        df['source'] = file_name\n",
    "        df= df[df['year'] > 1990]\n",
    "        df= df.drop_duplicates(['year','month','day'])\n",
    "        timeseries_dfs.append(df)\n",
    "    else:\n",
    "        df = df.rename({'ID':'station_id'}, axis=1)\n",
    "        df = df.set_index('station_id')\n",
    "        summary_dfs.append(df)\n",
    "\n",
    "timeseries_data = pd.concat(timeseries_dfs, axis=0, ignore_index=True)\n",
    "timeseries_data['date'] = pd.to_datetime(timeseries_data[['year', 'month', 'day']])\n",
    "timeseries_data = timeseries_data.drop(['year', 'month', 'day'], axis=1)\n",
    "\n",
    "summary_data = pd.concat(summary_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c59bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData():\n",
    "    def __init__(self, timeseries_data=timeseries_data, summary_data=summary_data):\n",
    "        ### Data Cleaning\n",
    "        self.timeseries_data = timeseries_data.replace(-99.99,np.NaN)\n",
    "        \n",
    "        ### Feature Engineering\n",
    "        # get precipitation deficit\n",
    "        actualTransEvap_data = self.timeseries_data[self.timeseries_data['source'] == 'et_morton_actual_SILO'].drop(['source'], axis=1)\n",
    "        precipitation_data = self.timeseries_data[self.timeseries_data['source'] == 'precipitation_AWAP'].drop(['source'], axis=1)\n",
    "         \n",
    "        actualTransEvap_data = actualTransEvap_data[actualTransEvap_data['date'].isin(precipitation_data['date'])].reset_index(drop=True)\n",
    "        precipitation_data = precipitation_data[precipitation_data['date'].isin(actualTransEvap_data['date'])].reset_index(drop=True)\n",
    "        \n",
    "        self.precipitation_deficit = precipitation_data.drop(['date'], axis=1).subtract(actualTransEvap_data.drop(['date'], axis=1))\n",
    "        self.precipitation_deficit['source'] = 'precipitation_deficit'\n",
    "        self.precipitation_deficit['date'] = precipitation_data['date']\n",
    "        \n",
    "        # get flood probabilities\n",
    "        self.streamflow_data = self.timeseries_data[timeseries_data['source'] == 'streamflow_MLd_inclInfilled'].drop(['source'], axis=1)\n",
    "        self.streamflow_data = self.streamflow_data.set_index('date')\n",
    "        \n",
    "        self.flood_probabilities = self.streamflow_data.apply(self.flood_extent, axis=0)\n",
    "        self.flood_probabilities['source'] = 'flood_probabilities'\n",
    "        self.flood_probabilities['date'] = self.streamflow_data.index\n",
    "        \n",
    "        self.flood_indicator = self.flood_probabilities.applymap(lambda x: int(x <0.05) if pd.isnull(x) == False and isinstance(x, float) else x)\n",
    "        self.flood_indicator['source'] = 'flood_indicator'\n",
    "        self.flood_indicator['date'] = self.flood_probabilities['date']        \n",
    "        \n",
    "        # turn date into sin and cos function \n",
    "        date_min = np.min(self.flood_probabilities['date'])\n",
    "        year_seconds = 365.2425*24*60*60\n",
    "        year_sin = self.flood_probabilities['date'].apply(lambda x: np.sin((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        year_cos = self.flood_probabilities['date'].apply(lambda x: np.cos((x-date_min).total_seconds() * (2 * np.pi / year_seconds)))\n",
    "        all_stations = list(self.flood_probabilities.drop(columns=['source', 'date'], axis=1).columns) \n",
    "        \n",
    "        df_sin = []     \n",
    "        for value in year_sin:\n",
    "            df_sin.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_sin = pd.DataFrame(df_sin)\n",
    "        df_sin['source'] = 'year_sin'\n",
    "        df_sin['date'] = self.flood_probabilities['date']\n",
    " \n",
    "        df_cos = []\n",
    "        for value in year_cos:\n",
    "            df_cos.append({k:value for k in all_stations})\n",
    "            \n",
    "        df_cos = pd.DataFrame(df_cos)\n",
    "        df_cos['source'] = 'year_cos'\n",
    "        df_cos['date'] = self.flood_probabilities['date']\n",
    "            \n",
    "        ### Return\n",
    "        self.timeseries_data = pd.concat([self.timeseries_data, self.precipitation_deficit, self.flood_probabilities, df_sin, df_cos, self.flood_indicator], axis=0).reset_index(drop=True)\n",
    "        self.summary_data = summary_data\n",
    "        \n",
    "    def get_timeseries_data(self, source, stations):      \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        return self.data_filtered\n",
    "        \n",
    "        \n",
    "    def get_data(self, source, stations):\n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]\n",
    "     \n",
    "        # filter by source\n",
    "        self.data_filtered = self.timeseries_data[self.timeseries_data['source'].isin(timeseries_source)]\n",
    "        # pivot data by station\n",
    "        self.data_filtered = self.data_filtered[['date', 'source'] + stations].pivot(index='date', columns='source', values=stations)\n",
    "        # get rows with no nan\n",
    "        self.data_filtered = self.data_filtered[~self.data_filtered.isnull().any(axis=1)]\n",
    "        \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                self.data_filtered[station, variable] = value\n",
    "        \n",
    "        return self.data_filtered.sort_index(axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_train_val_test(self, source, stations, \n",
    "                           scaled=True, target=['streamflow_MLd_inclInfilled'],\n",
    "                           start=None, end=None,\n",
    "                           discard=0.05, train=0.6, test=0.4):\n",
    "        assert 0<=discard<=1\n",
    "        assert (train + test) == 1\n",
    "     \n",
    "        summary_source = [i for i in source if i in list(self.summary_data.columns)]\n",
    "        timeseries_source = [i for i in source if i not in list(self.summary_data.columns)]        \n",
    "        \n",
    "        all_data = self.get_timeseries_data(timeseries_source, stations).loc[start:end]\n",
    "        n_rows_all = len(all_data)\n",
    "        \n",
    "        all_data_discarded = all_data.iloc[int(n_rows_all*discard):]\n",
    "        n_rows_discarded = len(all_data_discarded)\n",
    "        \n",
    "        train_df = all_data_discarded[:int(n_rows_discarded*train)]\n",
    "        test_df = all_data_discarded[-int(n_rows_discarded*(test)):]\n",
    "        \n",
    "        if scaled == True:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_df)\n",
    "            \n",
    "            scaler_test = MinMaxScaler()\n",
    "            scaler_test.fit(test_df)\n",
    "            \n",
    "            train_df = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "            test_df = pd.DataFrame(scaler_test.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "            \n",
    "     \n",
    "        for station in stations:\n",
    "            for variable in summary_source:\n",
    "                value = self.summary_data.loc[station][variable]\n",
    "                \n",
    "                train_df[station, variable] = value                \n",
    "                test_df[station, variable] = value \n",
    "                                  \n",
    "        return train_df.sort_index(axis=1), test_df.sort_index(axis=1) \n",
    "    \n",
    "    def flood_extent(self, streamflow_ts):\n",
    "        station_name = streamflow_ts.name\n",
    "\n",
    "        flow_data = pd.DataFrame(streamflow_ts)  \n",
    "        na_values = flow_data[flow_data[station_name].isna()][station_name]\n",
    "\n",
    "        flow_data = flow_data.dropna().sort_values(by=station_name, ascending=False).reset_index()\n",
    "        flow_data['probability'] = (flow_data.index + 1)/(1+len(flow_data)) \n",
    "        flow_data = flow_data.sort_values(by='date').drop(['date', station_name], axis=1)['probability']\n",
    "        flow_data = pd.concat([na_values, flow_data]).reset_index(drop=True) \n",
    "        flow_data.name = station_name  \n",
    "\n",
    "        return flow_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89db481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camels_data = PrepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, test_df, \n",
    "                 station, filtered=None, label_columns=None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df[station]\n",
    "        self.test_df = test_df[station]\n",
    "        self.station = station\n",
    "        self.filtered = filtered\n",
    "        \n",
    "        # validation\n",
    "        if self.filtered == 'upper_soil_filter':\n",
    "            assert('upper_soil_indicator' in list(self.train_df.columns))\n",
    "        \n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(self.train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, filtered=None, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length=self.total_window_size,\n",
    "                sequence_stride=1,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        \n",
    "        if filtered == 'upper_soil_filter':\n",
    "            indicator_index = list(self.train_df.columns).index('upper_soil_indicator')\n",
    "            ds = ds.unbatch().filter(lambda x, y: tf.math.reduce_sum(x[:, indicator_index]) > 0).batch(32)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df, filtered=self.filtered)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df, shuffle=False)\n",
    "           \n",
    "    @property\n",
    "    def test_windows(self):\n",
    "        total_size = self.test_array.shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    @property\n",
    "    def test_array(self):\n",
    "        return array(self.test_df[self.label_columns])\n",
    "       \n",
    "    def test_indicator(self, filtered):\n",
    "        if filtered == 'upper_soil_filter':\n",
    "            return array(self.test_df['upper_soil_indicator'])       \n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "            \n",
    "        return result\n",
    "       \n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self.train_df.shape[1]\n",
    "    \n",
    "    def test_example(self, index):\n",
    "        return np.array(self.test_df.iloc[index]).reshape(1, 1, self.num_features)\n",
    "\n",
    "    def plot(self, model=None, plot_col='streamflow_MLd_inclInfilled', max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                              marker='X', edgecolors='k', label='Predictions',\n",
    "                              c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ed6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df,test_df, stations, \n",
    "                 statics='separate', filtered=-1000, label_columns=None):\n",
    "     \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df       \n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations) \n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.filtered = filtered\n",
    "           \n",
    "        self.trains = []\n",
    "        self.tests = []        \n",
    "\n",
    "        for i, s in enumerate(stations):\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "\n",
    "            padding = np.zeros((input_width, self.total_stations), dtype=np.float32)\n",
    "            padding[:, i] = 1\n",
    "\n",
    "            self.trains.append(window.train.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            self.tests.append(window.test.unbatch().map(lambda x, y: (tf.concat([x, tf.convert_to_tensor(padding)], axis=1),y)))\n",
    "            \n",
    "    @property\n",
    "    def train(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.trains)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE).filter(lambda x, y: tf.math.reduce_max(y) > self.filtered)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.tests)\n",
    "        concat_ds = ds.interleave(lambda x: x, cycle_length=1, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return concat_ds.batch(32)\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])\n",
    "    \n",
    "    def station_percentile(self, cut, variable='streamflow_MLd_inclInfilled', station=None):\n",
    "        if station==None:\n",
    "            return np.percentile(self.test_df[variable], 100-cut)\n",
    "        else:\n",
    "            return np.percentile(self.test_df[station][variable], 100-cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb25c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNumpyWindow():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 timeseries_source, summary_source, summary_data,\n",
    "                 stations, label_columns=None):\n",
    "        \n",
    "        train_df, test_df = camels_data.get_train_val_test(source=timeseries_source,\n",
    "                                                                   stations=stations)      \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.stations = stations\n",
    "        self.total_stations = len(stations)\n",
    "        \n",
    "        self.num_timeseries_features = len(timeseries_source)\n",
    "        self.num_static_features = len(summary_source)     \n",
    "        self.timeseries_source = timeseries_source\n",
    "        self.summary_source = summary_source      \n",
    "    \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "        self.label_columns = label_columns\n",
    "        \n",
    "        self.train_timeseries = []\n",
    "        self.test_timeseries = []  \n",
    "        \n",
    "        self.train_static = []\n",
    "        self.test_static = [] \n",
    "        \n",
    "        self.train_y = []\n",
    "        self.test_y = []\n",
    "        \n",
    "        for i, s in enumerate(stations):\n",
    "            # process timeseries\n",
    "            window = WindowGenerator(input_width=input_width,\n",
    "                                     label_width=label_width,\n",
    "                                     shift=shift,\n",
    "                                     train_df=train_df,\n",
    "                                     test_df=test_df,\n",
    "                                     station=s,\n",
    "                                     label_columns=label_columns)\n",
    "            \n",
    "            x_train, y_train = self.mapdata_tonumpy(window.train)\n",
    "            x_test, y_test = self.mapdata_tonumpy(window.test)\n",
    "            \n",
    "            self.train_timeseries.extend(x_train) \n",
    "            self.test_timeseries.extend(x_test) \n",
    "            \n",
    "            self.train_y.extend(y_train) \n",
    "            self.test_y.extend(y_test)      \n",
    "            \n",
    "            # process static\n",
    "            static = summary_data[summary_source].loc[s].to_numpy()         \n",
    "            padding = np.zeros((self.total_stations, ), dtype=np.float32)\n",
    "            padding[i] = 1\n",
    "            \n",
    "            static = np.concatenate([static, padding], axis=0)\n",
    "            \n",
    "            self.train_static.extend([static for _ in range(x_train.shape[0])])\n",
    "            self.test_static.extend([static for _ in range(x_test.shape[0])])\n",
    "            \n",
    "        self.train_timeseries = np.array(self.train_timeseries)\n",
    "        self.test_timeseries = np.array(self.test_timeseries) \n",
    "        \n",
    "        self.train_static = np.array(self.train_static)\n",
    "        self.test_static = np.array(self.test_static)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.train_static)\n",
    "        \n",
    "        self.train_static = scaler.transform(self.train_static)\n",
    "        self.test_static = scaler.transform(self.test_static)\n",
    "                \n",
    "        \n",
    "        #self.train_static = np.ones((self.train_static.shape[0], self.train_static.shape[1]))\n",
    "        \n",
    "        self.train_y = np.array(self.train_y) \n",
    "        self.train_y = np.swapaxes(self.train_y, 1, 2)\n",
    "\n",
    "        \n",
    "        self.test_y = np.array(self.test_y)\n",
    "        self.test_y = np.swapaxes(self.test_y, 1, 2)        \n",
    "\n",
    "    def mapdata_tonumpy(self, map_ds):\n",
    "        map_ds = map_ds.unbatch()\n",
    "\n",
    "        x_array = []\n",
    "        y_array = []\n",
    "\n",
    "        for ts in map_ds:\n",
    "            x = ts[0]\n",
    "            y= ts[1]\n",
    "\n",
    "            x_array.append(x.numpy())\n",
    "            y_array.append(y.numpy())\n",
    "\n",
    "        return np.array(x_array), np.array(y_array) \n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.train_timeseries, self.train_static, self.train_y\n",
    "\n",
    "\n",
    "    @property\n",
    "    def test(self):      \n",
    "        return self.test_timeseries, self.test_static, self.test_y\n",
    "\n",
    "\n",
    "    def test_windows(self, station=None):\n",
    "        total_size = self.test_array(station).shape[0]\n",
    "        convolution_size = self.input_width\n",
    "        prediction_size = self.label_width\n",
    "        \n",
    "        a = []\n",
    "        \n",
    "        for i in range(convolution_size, (total_size-prediction_size+1)):\n",
    "            index_list = list(range(i, i+prediction_size))\n",
    "            a.append(self.test_array(station)[index_list])\n",
    "        \n",
    "        return np.squeeze(array(a), axis=2)\n",
    "    \n",
    "    def test_array(self, station):\n",
    "        return array(self.test_df[station][self.label_columns])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
